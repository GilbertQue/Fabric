{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3d37402-b20f-4405-aba6-eef770a66dd9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2024-10-21T23:47:28.8635651Z",
       "execution_start_time": "2024-10-21T23:47:25.9852813Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "630e64a5-4bd0-481f-accb-b11600beeb83",
       "queued_time": "2024-10-21T23:47:16.4184774Z",
       "session_id": "8d7fd7aa-c188-42f3-9836-b01257d95e1b",
       "session_start_time": "2024-10-21T23:47:16.7203968Z",
       "spark_pool": null,
       "state": "finished",
       "statement_id": 3,
       "statement_ids": [
        3
       ]
      },
      "text/plain": [
       "StatementMeta(, 8d7fd7aa-c188-42f3-9836-b01257d95e1b, 3, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reference https://fabric.guru/programmatically-creating-managing-lakehouses-in-fabric\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from requests.exceptions import HTTPError\n",
    "\n",
    "def get_lakehouse_list_api():\n",
    "\n",
    "    '''\n",
    "    Sandeep Pawar  |   fabric.guru\n",
    "\n",
    "    This function uses the Fabric REST API to get all the lakehouses in the tenant that the user has access to.\n",
    "\n",
    "    '''\n",
    "    base_url = \"https://api.fabric.microsoft.com/v1/admin/items?type=Lakehouse\"\n",
    "    token = mssparkutils.credentials.getToken(\"https://api.fabric.microsoft.com/\")\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "\n",
    "    try:\n",
    "        response = requests.get(base_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        data = response.json()\n",
    "\n",
    "        # Check if 'itemEntities' exists, may not exist for all items types\n",
    "        if 'itemEntities' not in data:\n",
    "            raise KeyError(\"'itemEntities' key not found in the response data\")\n",
    "\n",
    "        lakehouses = pd.json_normalize(data['itemEntities'], sep='_')\n",
    "        return lakehouses\n",
    "\n",
    "    except HTTPError as http_err:\n",
    "        print(f\"HTTP error occurred: {http_err}\")\n",
    "    except Exception as err:\n",
    "        print(f\"An error occurred: {err}\")\n",
    "\n",
    "df_Lakehouses = get_lakehouse_list_api()\n",
    "#display(df_Lakehouses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c3e06a2-3b8e-4247-9a10-ec4051d9e240",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2024-10-21T23:55:54.635247Z",
       "execution_start_time": "2024-10-21T23:55:54.3365658Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "1bae303b-4106-447c-a5ba-c3ae6d92510f",
       "queued_time": "2024-10-21T23:55:53.6720362Z",
       "session_id": "8d7fd7aa-c188-42f3-9836-b01257d95e1b",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 4,
       "statement_ids": [
        4
       ]
      },
      "text/plain": [
       "StatementMeta(, 8d7fd7aa-c188-42f3-9836-b01257d95e1b, 4, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Code generated by Data Wrangler for pandas DataFrame\n",
    "# Get Lakehouse ID, Lakehouse Name and Workspace ID\n",
    "# Exclude Dataflows Staging\n",
    "\n",
    "def clean_data(df_Lakehouses):\n",
    "    # Filter rows based on column: 'name'\n",
    "    df_Lakehouses = df_Lakehouses[~df_Lakehouses['name'].str.contains(\"DataflowsStaging\", regex=False, na=False)]\n",
    "    return df_Lakehouses\n",
    "\n",
    "df_Lakehouses_clean = clean_data(df_Lakehouses.copy())\n",
    "#df_Lakehouses_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18245001-fc8e-42a8-abb5-ba9288bbfa7a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2024-10-08T00:38:21.7826302Z",
       "execution_start_time": "2024-10-08T00:38:21.0097941Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "d929b50d-b3ae-46aa-9d4c-2651530f6c06",
       "queued_time": "2024-10-08T00:38:18.3856205Z",
       "session_id": "2a870d78-2948-4a5b-bc59-32c45802ca9e",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 6,
       "statement_ids": [
        6
       ]
      },
      "text/plain": [
       "StatementMeta(, 2a870d78-2948-4a5b-bc59-32c45802ca9e, 6, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Warehouses\n",
    "# Reference https://fabric.guru/programmatically-creating-managing-lakehouses-in-fabric\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from requests.exceptions import HTTPError\n",
    "\n",
    "def get_warehouse_list_api():\n",
    "\n",
    "    '''\n",
    "    Sandeep Pawar  |   fabric.guru\n",
    "\n",
    "    This function uses the Fabric REST API to get all the lakehouses in the tenant that the user has access to.\n",
    "\n",
    "    '''\n",
    "    base_url = \"https://api.fabric.microsoft.com/v1/admin/items?type=Warehouse\"\n",
    "    token = mssparkutils.credentials.getToken(\"https://api.fabric.microsoft.com/\")\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "\n",
    "    try:\n",
    "        response = requests.get(base_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        data = response.json()\n",
    "\n",
    "        # Check if 'itemEntities' exists, may not exist for all items types\n",
    "        if 'itemEntities' not in data:\n",
    "            raise KeyError(\"'itemEntities' key not found in the response data\")\n",
    "\n",
    "        lakehouses = pd.json_normalize(data['itemEntities'], sep='_')\n",
    "        return lakehouses\n",
    "\n",
    "    except HTTPError as http_err:\n",
    "        print(f\"HTTP error occurred: {http_err}\")\n",
    "    except Exception as err:\n",
    "        print(f\"An error occurred: {err}\")\n",
    "\n",
    "df_Warehouses = get_warehouse_list_api()\n",
    "#display(df_Warehouses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4953148a-93da-4b30-b4bb-ae5f32440236",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2024-10-08T00:38:22.7055408Z",
       "execution_start_time": "2024-10-08T00:38:22.3659482Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "2ff877cc-1538-4409-b281-4dbcf8b45530",
       "queued_time": "2024-10-08T00:38:18.4497198Z",
       "session_id": "2a870d78-2948-4a5b-bc59-32c45802ca9e",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 7,
       "statement_ids": [
        7
       ]
      },
      "text/plain": [
       "StatementMeta(, 2a870d78-2948-4a5b-bc59-32c45802ca9e, 7, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Code generated by Data Wrangler for pandas DataFrame for Warehouses\n",
    "# Get Lakehouse ID, Lakehouse Name and Workspace ID\n",
    "# Exclude Dataflows Staging\n",
    "\n",
    "def clean_data(df_Warehouses):\n",
    "    # Filter rows based on column: 'name'\n",
    "    df_Warehouses = df_Warehouses[~df_Warehouses['name'].str.contains(\"DataflowsStaging\", regex=False, na=False)]\n",
    "    return df_Warehouses\n",
    "\n",
    "df_Warehouses_clean = clean_data(df_Warehouses.copy())\n",
    "#df_Warehouses_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba71ce32-2b71-4e28-a946-f43fefd3bfe4",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2024-10-08T00:38:24.8650915Z",
       "execution_start_time": "2024-10-08T00:38:23.2744792Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "6e36cfed-179c-4dff-a154-1d6a08df39b4",
       "queued_time": "2024-10-08T00:38:18.4964226Z",
       "session_id": "2a870d78-2948-4a5b-bc59-32c45802ca9e",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 8,
       "statement_ids": [
        8
       ]
      },
      "text/plain": [
       "StatementMeta(, 2a870d78-2948-4a5b-bc59-32c45802ca9e, 8, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection with OneLake is successful.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from azure.identity import ClientSecretCredential, AuthenticationRequiredError\n",
    "\n",
    "def get_access_token(app_id, client_secret, directory_id):\n",
    "    try:\n",
    "        # Create the ClientSecretCredential using the provided credentials\n",
    "        # How and where did I find the blog post to enable the App Registration with the valid permissions        \n",
    "\n",
    "        ######################################################################################### \n",
    "        # Read secretes from Azure Key Vault\n",
    "        #########################################################################################\n",
    "        ## This is the name of my Azure Key Vault \n",
    "        key_vault = \"https://domain-keyvault.vault.azure.net/\"\n",
    "        ## I have stored my tenant id as one of the secrets to make it easier to use when needed \n",
    "        tenant = mssparkutils.credentials.getSecret(key_vault , \"tenantid\") \n",
    "        ## This is my application Id for my service principal account \n",
    "        client = mssparkutils.credentials.getSecret(key_vault , \"pbi-applicationid\") \n",
    "        ## This is my Client Secret for my service principal account \n",
    "        clientsecret = mssparkutils.credentials.getSecret(key_vault , \"powerbi-clientsecret\") \n",
    "\n",
    "        # I need to show in the permissions where I selected the Azure Storage.\n",
    "        credential = ClientSecretCredential(\n",
    "            client_id= client,\n",
    "            client_secret= clientsecret,\n",
    "            tenant_id= tenant\n",
    "        ) \n",
    "\n",
    "        # Use the credential to get the access token\n",
    "        token = credential.get_token(\"https://storage.azure.com/.default\").token\n",
    "\n",
    "        return token, credential\n",
    "\n",
    "    except AuthenticationRequiredError as e:\n",
    "        print(\"Authentication failed. Please check your credentials.\")\n",
    "        raise e\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred while getting the access token:\")\n",
    "        print(str(e))\n",
    "        raise e\n",
    "    \n",
    "access_token, credential = get_access_token(\"appId\", \"secret\", \"tenantId\")\n",
    "\n",
    "\n",
    "def check_connection_with_onelake(access_token):\n",
    "    base_url = \"https://onelake.dfs.fabric.microsoft.com/WORKSPACE_GUID/LAKEHOUSE_GUID/Files/\" \n",
    "    token_headers = {\n",
    "        \"Authorization\": \"Bearer \" + access_token\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(base_url, headers=token_headers)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            print(\"Connection with OneLake is successful.\")\n",
    "        else:\n",
    "            print(\"Failed to connect with OneLake. Status code:\", response.status_code)\n",
    "            print(response.content)\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"An error occurred while checking the connection:\", str(e))\n",
    "\n",
    "# Assuming 'access_token' is already defined and contains a valid access token\n",
    "check_connection_with_onelake(access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "836dde9c-132f-42a1-9b3b-796e03c92547",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2024-10-08T00:38:46.4355813Z",
       "execution_start_time": "2024-10-08T00:38:27.7123193Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "0bcf5583-8af0-40fe-97cb-619042ccbd6a",
       "queued_time": "2024-10-08T00:38:18.703776Z",
       "session_id": "2a870d78-2948-4a5b-bc59-32c45802ca9e",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 12,
       "statement_ids": [
        12
       ]
      },
      "text/plain": [
       "StatementMeta(, 2a870d78-2948-4a5b-bc59-32c45802ca9e, 12, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loop Through Data for Files\n",
    "import datetime\n",
    "from pyspark.sql.types import IntegerType,BooleanType,DateType\n",
    "from pyspark.sql.functions import col, year, month, quarter\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import lit\n",
    "import requests\n",
    "import json\n",
    "from pandas import json_normalize\n",
    "from pyspark.sql import types as T \n",
    "from pyspark.sql import SparkSession \n",
    "\n",
    "# Create a Spark session\n",
    "sparkdf = spark.createDataFrame(df_Lakehouses_clean)\n",
    "spark = SparkSession.builder.appName(\"AddColumnExample\").getOrCreate()\n",
    "data_collect = sparkdf.collect()\n",
    "\n",
    "# looping thorough each row of the dataframe\n",
    "\n",
    "for row in data_collect:\n",
    "    # Variables Below\n",
    "    var_lakehouseId = row[\"id\"]\n",
    "    var_lakehouseName = row[\"name\"]\n",
    "    var_workspaceId = row[\"workspaceId\"]\n",
    "\n",
    "    try:\n",
    "        \n",
    "        # Get Files Details\n",
    "        mssparkutils.credentials.getToken('storage')\n",
    "        get_url = 'https://onelake.dfs.fabric.microsoft.com/'+ var_workspaceId +'/'+ var_lakehouseId +'/Files?recursive=True&resource=filesystem'\n",
    "        # display(get_url)\n",
    "        resp = requests.get(get_url, headers={\"authorization\":f\"bearer {mssparkutils.credentials.getToken('storage')}\"})\n",
    "        mydata = json.loads(resp.text)\n",
    "\n",
    "        # Convert to Data Frame\n",
    "        pandas_df = json_normalize(mydata, 'paths')\n",
    "\n",
    "        # Reference for DateTime Formats https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior\n",
    "        pandas_df['lastModified'] = pd.to_datetime(pandas_df.lastModified, format = \"%a, %d %b %Y %H:%M:%S %Z\" )\n",
    "\n",
    "        #Convert to Spark Data Frame \n",
    "        df=spark.createDataFrame(pandas_df) \n",
    "\n",
    "        #Additional Columns are added \n",
    "        df = df.withColumn(\"LakehouseId\" , lit(var_lakehouseId)) \n",
    "        df = df.withColumn(\"LakehouseName\" , lit(var_lakehouseName)) \n",
    "        df = df.withColumn(\"WorkspaceId\" , lit(var_workspaceId)) \n",
    "        df = df.withColumn('contentLength', df['contentLength'].cast(T.LongType())) \n",
    "        df = df.withColumn('lastModified', df['lastModified'].cast(T.TimestampType()))\n",
    "        df = df.withColumn(\"creationTime\", from_unixtime(df.creationTime / 1e9).cast(T.TimestampType()))\n",
    "        df = df.withColumn(\"CurrentDateTime\", lit(current_timestamp()))\n",
    "        df = df.withColumn('Year', year(current_timestamp()))\n",
    "        df = df.withColumn('Month', month(current_timestamp()))\n",
    "        df = df.withColumn('Day', dayofmonth(current_timestamp()))\n",
    "                       \n",
    "        #Writing to Lake House\n",
    "        df.write.mode(\"append\").format(\"delta\").partitionBy(\"Year\",\"Month\",\"Day\").save(\"Tables/Onelake_Storage_Files\") \n",
    "\n",
    "    except Exception as e:\n",
    "        Error: {e}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fd7ad85-1fea-47fc-9bcb-9fbfa2082503",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2024-10-08T00:38:59.1546994Z",
       "execution_start_time": "2024-10-08T00:38:47.0710834Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "7e68b16e-3012-44a2-bdf2-18d1e86e74d1",
       "queued_time": "2024-10-08T00:38:18.7455475Z",
       "session_id": "2a870d78-2948-4a5b-bc59-32c45802ca9e",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 13,
       "statement_ids": [
        13
       ]
      },
      "text/plain": [
       "StatementMeta(, 2a870d78-2948-4a5b-bc59-32c45802ca9e, 13, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loop Through Data for Tables\n",
    "import datetime\n",
    "from pyspark.sql.types import IntegerType,BooleanType,DateType\n",
    "from pyspark.sql.functions import col, year, month, quarter\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import lit\n",
    "import requests\n",
    "import json\n",
    "from pandas import json_normalize\n",
    "from pyspark.sql import types as T \n",
    "from pyspark.sql import SparkSession \n",
    "\n",
    "# Create a Spark session\n",
    "sparkdf = spark.createDataFrame(df_Lakehouses_clean)\n",
    "spark = SparkSession.builder.appName(\"AddColumnExample\").getOrCreate()\n",
    "data_collect = sparkdf.collect()\n",
    "\n",
    "# looping thorough each row of the dataframe\n",
    "\n",
    "for row in data_collect:\n",
    "#\n",
    "\n",
    "    var_lakehouseId = row[\"id\"]\n",
    "    var_lakehouseName = row[\"name\"]\n",
    "    var_workspaceId = row[\"workspaceId\"]\n",
    "\n",
    "    try:\n",
    "        \n",
    "        # Reference https://www.reddit.com/r/MicrosoftFabric/comments/1bnaz0a/api_to_fetch_list_of_files_in_a_onelake_folder/\n",
    "        mssparkutils.credentials.getToken('storage')\n",
    "        get_url = 'https://onelake.dfs.fabric.microsoft.com/'+ var_workspaceId +'/'+ var_lakehouseId +'/Tables?recursive=True&resource=filesystem'\n",
    "        # display(get_url)\n",
    "        resp = requests.get(get_url, headers={\"authorization\":f\"bearer {mssparkutils.credentials.getToken('storage')}\"})\n",
    "        mydata = json.loads(resp.text)\n",
    "\n",
    "        # Convert to Data Frame\n",
    "        pandas_df = json_normalize(mydata, 'paths')\n",
    "\n",
    "        # Reference for DateTime Formats https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior\n",
    "        pandas_df['lastModified'] = pd.to_datetime(pandas_df.lastModified, format = \"%a, %d %b %Y %H:%M:%S %Z\" )\n",
    "\n",
    "        #Convert to Spark Data Frame \n",
    "\n",
    "        df=spark.createDataFrame(pandas_df) \n",
    "\n",
    "        #Additional Columns are added \n",
    "        df = df.withColumn(\"LakehouseId\" , lit(var_lakehouseId)) \n",
    "        df = df.withColumn(\"LakehouseName\" , lit(var_lakehouseName)) \n",
    "        df = df.withColumn(\"WorkspaceId\" , lit(var_workspaceId)) \n",
    "        \n",
    "\n",
    "        df = df.withColumn('contentLength', df['contentLength'].cast(T.LongType())) \n",
    "        df = df.withColumn('lastModified', df['lastModified'].cast(T.TimestampType()))\n",
    "        df = df.withColumn(\"creationTime\", from_unixtime(df.creationTime / 1e9).cast(T.TimestampType()))\n",
    "        df = df.withColumn(\"CurrentDateTime\", lit(current_timestamp()))\n",
    "        df = df.withColumn('Year', year(current_timestamp()))\n",
    "        df = df.withColumn('Month', month(current_timestamp()))\n",
    "        df = df.withColumn('Day', dayofmonth(current_timestamp()))\n",
    "        # Bulk Rename column Names replacing \"]\" with \"\"\n",
    "               \n",
    "        #Writing to Lake House\n",
    "        df.write.mode(\"append\").format(\"delta\").partitionBy(\"Year\",\"Month\",\"Day\").save(\"Tables/Onelake_Storage_Tables\") \n",
    "\n",
    "    except Exception as e:\n",
    "        Error: {e}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce05ab75-f2c0-41ca-bf64-727a675c8159",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2024-10-08T00:39:06.0184736Z",
       "execution_start_time": "2024-10-08T00:38:59.7392775Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "03834306-f548-4c68-ae92-c1d828c9e8ce",
       "queued_time": "2024-10-08T00:38:18.7905147Z",
       "session_id": "2a870d78-2948-4a5b-bc59-32c45802ca9e",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 14,
       "statement_ids": [
        14
       ]
      },
      "text/plain": [
       "StatementMeta(, 2a870d78-2948-4a5b-bc59-32c45802ca9e, 14, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loop Through Data for Warehouses\n",
    "import datetime\n",
    "from pyspark.sql.types import IntegerType,BooleanType,DateType\n",
    "from pyspark.sql.functions import col, year, month, quarter\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import lit\n",
    "import requests\n",
    "import json\n",
    "from pandas import json_normalize\n",
    "from pyspark.sql import types as T \n",
    "from pyspark.sql import SparkSession \n",
    "\n",
    "# Create a Spark session\n",
    "sparkdf = spark.createDataFrame(df_Warehouses_clean)\n",
    "spark = SparkSession.builder.appName(\"AddColumnExample\").getOrCreate()\n",
    "data_collect = sparkdf.collect()\n",
    "\n",
    "# looping thorough each row of the dataframe\n",
    "\n",
    "for row in data_collect:\n",
    "#\n",
    "\n",
    "    var_lakehouseId = row[\"id\"]\n",
    "    var_lakehouseName = row[\"name\"]\n",
    "    var_workspaceId = row[\"workspaceId\"]\n",
    "\n",
    "    try:\n",
    "        \n",
    "        # Reference https://www.reddit.com/r/MicrosoftFabric/comments/1bnaz0a/api_to_fetch_list_of_files_in_a_onelake_folder/\n",
    "        mssparkutils.credentials.getToken('storage')\n",
    "        get_url = 'https://onelake.dfs.fabric.microsoft.com/'+ var_workspaceId +'/'+ var_lakehouseId +'/Tables?recursive=True&resource=filesystem'\n",
    "        # display(get_url)\n",
    "        resp = requests.get(get_url, headers={\"authorization\":f\"bearer {mssparkutils.credentials.getToken('storage')}\"})\n",
    "        mydata = json.loads(resp.text)\n",
    "\n",
    "        # Convert to Data Frame\n",
    "        pandas_df = json_normalize(mydata, 'paths')\n",
    "\n",
    "        # Reference for DateTime Formats https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior\n",
    "        pandas_df['lastModified'] = pd.to_datetime(pandas_df.lastModified, format = \"%a, %d %b %Y %H:%M:%S %Z\" )\n",
    "\n",
    "        #Convert to Spark Data Frame \n",
    "\n",
    "        df=spark.createDataFrame(pandas_df) \n",
    "\n",
    "        #Additional Columns are added \n",
    "        df = df.withColumn(\"LakehouseId\" , lit(var_lakehouseId)) \n",
    "        df = df.withColumn(\"LakehouseName\" , lit(var_lakehouseName)) \n",
    "        df = df.withColumn(\"WorkspaceId\" , lit(var_workspaceId)) \n",
    "        \n",
    "\n",
    "        df = df.withColumn('contentLength', df['contentLength'].cast(T.LongType())) \n",
    "        df = df.withColumn('lastModified', df['lastModified'].cast(T.TimestampType()))\n",
    "        df = df.withColumn(\"creationTime\", from_unixtime(df.creationTime / 1e9).cast(T.TimestampType()))\n",
    "        df = df.withColumn(\"CurrentDateTime\", lit(current_timestamp()))\n",
    "        df = df.withColumn('Year', year(current_timestamp()))\n",
    "        df = df.withColumn('Month', month(current_timestamp()))\n",
    "        df = df.withColumn('Day', dayofmonth(current_timestamp()))\n",
    "        # Bulk Rename column Names replacing \"]\" with \"\"\n",
    "               \n",
    "        #Writing to Lake House\n",
    "        df.write.mode(\"append\").format(\"delta\").partitionBy(\"Year\",\"Month\",\"Day\").save(\"Tables/Onelake_Storage_Warehouses\") \n",
    "\n",
    "    except Exception as e:\n",
    "        Error: {e}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447e4a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading Workspace Details\n",
    "import json, requests, pandas as pd \n",
    "import datetime \n",
    "from datetime import datetime,date,timedelta \n",
    "\n",
    "######################################################################################### \n",
    "# Read secretes from Azure Key Vault\n",
    "#########################################################################################\n",
    "## This is the name of my Azure Key Vault \n",
    "key_vault = \"https://domain-keyvault.vault.azure.net/\"\n",
    "## I have stored my tenant id as one of the secrets to make it easier to use when needed \n",
    "tenant = mssparkutils.credentials.getSecret(key_vault , \"tenantid\") \n",
    "## This is my application Id for my service principal account \n",
    "client = mssparkutils.credentials.getSecret(key_vault , \"pbi-applicationid\") \n",
    "## This is my Client Secret for my service principal account \n",
    "client_secret = mssparkutils.credentials.getSecret(key_vault , \"powerbi-clientsecret\") \n",
    "\n",
    "######################################################################################### \n",
    "# Authentication - Replace string variables with your relevant values \n",
    "#########################################################################################  \n",
    "\n",
    "import json, requests, pandas as pd \n",
    "import datetime  \n",
    "\n",
    "try: \n",
    "    from azure.identity import ClientSecretCredential \n",
    "except Exception:\n",
    "     !pip install azure.identity \n",
    "     from azure.identity import ClientSecretCredential \n",
    "\n",
    "# Generates the access token for the Service Principal \n",
    "api = 'https://analysis.windows.net/powerbi/api/.default' \n",
    "auth = ClientSecretCredential(authority = 'https://login.microsoftonline.com/', \n",
    "               tenant_id = tenant, \n",
    "               client_id = client, \n",
    "               client_secret = client_secret) \n",
    "access_token = auth.get_token(api)\n",
    "access_token = access_token.token \n",
    "\n",
    "## This is where I store my header with the Access Token, because this is required when authenticating \n",
    "## to the Power BI Admin APIs \n",
    "header = {'Authorization': f'Bearer {access_token}'}  \n",
    "\n",
    "print('\\nSuccessfully authenticated.')\n",
    "\n",
    "header = {'Authorization': f'Bearer {access_token}'} \n",
    "\n",
    "## Below is an example where I am querying the datasets Admin API and I need the access token in the\n",
    "## headers \n",
    "base_url = 'https://api.powerbi.com/v1.0/myorg/admin/' \n",
    "refreshables_url = \"groups?%24top=5000\"  \n",
    "workspace_response = requests.get(base_url + refreshables_url, headers=header)\n",
    "\n",
    "# Get the response into a DataFrame\n",
    "data_workspaces = workspace_response.json()['value']\n",
    "\n",
    "df_workspaces = pd.DataFrame(data_workspaces)\n",
    "# print(df_app_users)\n",
    "\n",
    "# #Create SparkDataFrame\n",
    "spark_workspaces = spark.createDataFrame(df_workspaces)\n",
    "\n",
    "# Select specific columns\n",
    "selected_spark_workspaces = spark_workspaces.select('id', 'isReadOnly','isOnDedicatedCapacity','description','type','state','name','capacityId','defaultDatasetStorageFormat','dataflowStorageId','pipelineId')\n",
    "\n",
    "# Rename columns\n",
    "selected_spark_workspaces = selected_spark_workspaces.withColumnRenamed(\"id\", \"WorkspaceId\")\n",
    "selected_spark_workspaces = selected_spark_workspaces.withColumnRenamed(\"name\", \"WorkspaceName\")\n",
    "\n",
    "#display(spark_workspaces)\n",
    "\n",
    "# Write to Lakehouse Table\n",
    "selected_spark_workspaces.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").format(\"delta\").save(f\"Tables/WorkspaceDetails\")"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "260cc545-9a59-45a7-9897-fb705d4bc4ae",
    "default_lakehouse_name": "FM_LH",
    "default_lakehouse_workspace_id": "cb2af739-998e-45c8-8c41-f78d2e8fc1dd",
    "known_lakehouses": [
     {
      "id": "260cc545-9a59-45a7-9897-fb705d4bc4ae"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default"
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
