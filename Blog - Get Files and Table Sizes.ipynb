{"cells":[{"cell_type":"code","source":["# Reference https://fabric.guru/programmatically-creating-managing-lakehouses-in-fabric\n","\n","import requests\n","import pandas as pd\n","from requests.exceptions import HTTPError\n","\n","def get_lakehouse_list_api():\n","\n","    '''\n","    Sandeep Pawar  |   fabric.guru\n","\n","    This function uses the Fabric REST API to get all the lakehouses in the tenant that the user has access to.\n","\n","    '''\n","    base_url = \"https://api.fabric.microsoft.com/v1/admin/items?type=Lakehouse\"\n","    token = mssparkutils.credentials.getToken(\"https://api.fabric.microsoft.com/\")\n","    headers = {\"Authorization\": f\"Bearer {token}\"}\n","\n","    try:\n","        response = requests.get(base_url, headers=headers)\n","        response.raise_for_status()\n","\n","        data = response.json()\n","\n","        # Check if 'itemEntities' exists, may not exist for all items types\n","        if 'itemEntities' not in data:\n","            raise KeyError(\"'itemEntities' key not found in the response data\")\n","\n","        lakehouses = pd.json_normalize(data['itemEntities'], sep='_')\n","        return lakehouses\n","\n","    except HTTPError as http_err:\n","        print(f\"HTTP error occurred: {http_err}\")\n","    except Exception as err:\n","        print(f\"An error occurred: {err}\")\n","\n","df_Lakehouses = get_lakehouse_list_api()\n","#display(df_Lakehouses)\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"8d7fd7aa-c188-42f3-9836-b01257d95e1b","normalized_state":"finished","queued_time":"2024-10-21T23:47:16.4184774Z","session_start_time":"2024-10-21T23:47:16.7203968Z","execution_start_time":"2024-10-21T23:47:25.9852813Z","execution_finish_time":"2024-10-21T23:47:28.8635651Z","parent_msg_id":"630e64a5-4bd0-481f-accb-b11600beeb83"},"text/plain":"StatementMeta(, 8d7fd7aa-c188-42f3-9836-b01257d95e1b, 3, Finished, Available, Finished)"},"metadata":{}}],"execution_count":1,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false,"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d3d37402-b20f-4405-aba6-eef770a66dd9"},{"cell_type":"code","source":["# Code generated by Data Wrangler for pandas DataFrame\n","# Get Lakehouse ID, Lakehouse Name and Workspace ID\n","# Exclude Dataflows Staging\n","\n","def clean_data(df_Lakehouses):\n","    # Filter rows based on column: 'name'\n","    df_Lakehouses = df_Lakehouses[~df_Lakehouses['name'].str.contains(\"DataflowsStaging\", regex=False, na=False)]\n","    return df_Lakehouses\n","\n","df_Lakehouses_clean = clean_data(df_Lakehouses.copy())\n","#df_Lakehouses_clean.head()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"8d7fd7aa-c188-42f3-9836-b01257d95e1b","normalized_state":"finished","queued_time":"2024-10-21T23:55:53.6720362Z","session_start_time":null,"execution_start_time":"2024-10-21T23:55:54.3365658Z","execution_finish_time":"2024-10-21T23:55:54.635247Z","parent_msg_id":"1bae303b-4106-447c-a5ba-c3ae6d92510f"},"text/plain":"StatementMeta(, 8d7fd7aa-c188-42f3-9836-b01257d95e1b, 4, Finished, Available, Finished)"},"metadata":{}}],"execution_count":2,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1c3e06a2-3b8e-4247-9a10-ec4051d9e240"},{"cell_type":"code","source":["# Warehouses\n","# Reference https://fabric.guru/programmatically-creating-managing-lakehouses-in-fabric\n","\n","import requests\n","import pandas as pd\n","from requests.exceptions import HTTPError\n","\n","def get_warehouse_list_api():\n","\n","    '''\n","    Sandeep Pawar  |   fabric.guru\n","\n","    This function uses the Fabric REST API to get all the lakehouses in the tenant that the user has access to.\n","\n","    '''\n","    base_url = \"https://api.fabric.microsoft.com/v1/admin/items?type=Warehouse\"\n","    token = mssparkutils.credentials.getToken(\"https://api.fabric.microsoft.com/\")\n","    headers = {\"Authorization\": f\"Bearer {token}\"}\n","\n","    try:\n","        response = requests.get(base_url, headers=headers)\n","        response.raise_for_status()\n","\n","        data = response.json()\n","\n","        # Check if 'itemEntities' exists, may not exist for all items types\n","        if 'itemEntities' not in data:\n","            raise KeyError(\"'itemEntities' key not found in the response data\")\n","\n","        lakehouses = pd.json_normalize(data['itemEntities'], sep='_')\n","        return lakehouses\n","\n","    except HTTPError as http_err:\n","        print(f\"HTTP error occurred: {http_err}\")\n","    except Exception as err:\n","        print(f\"An error occurred: {err}\")\n","\n","df_Warehouses = get_warehouse_list_api()\n","#display(df_Warehouses)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","session_id":"2a870d78-2948-4a5b-bc59-32c45802ca9e","normalized_state":"finished","queued_time":"2024-10-08T00:38:18.3856205Z","session_start_time":null,"execution_start_time":"2024-10-08T00:38:21.0097941Z","execution_finish_time":"2024-10-08T00:38:21.7826302Z","parent_msg_id":"d929b50d-b3ae-46aa-9d4c-2651530f6c06"},"text/plain":"StatementMeta(, 2a870d78-2948-4a5b-bc59-32c45802ca9e, 6, Finished, Available, Finished)"},"metadata":{}}],"execution_count":4,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false,"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"18245001-fc8e-42a8-abb5-ba9288bbfa7a"},{"cell_type":"code","source":["# Code generated by Data Wrangler for pandas DataFrame for Warehouses\n","# Get Lakehouse ID, Lakehouse Name and Workspace ID\n","# Exclude Dataflows Staging\n","\n","def clean_data(df_Warehouses):\n","    # Filter rows based on column: 'name'\n","    df_Warehouses = df_Warehouses[~df_Warehouses['name'].str.contains(\"DataflowsStaging\", regex=False, na=False)]\n","    return df_Warehouses\n","\n","df_Warehouses_clean = clean_data(df_Warehouses.copy())\n","#df_Warehouses_clean.head()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":7,"statement_ids":[7],"state":"finished","livy_statement_state":"available","session_id":"2a870d78-2948-4a5b-bc59-32c45802ca9e","normalized_state":"finished","queued_time":"2024-10-08T00:38:18.4497198Z","session_start_time":null,"execution_start_time":"2024-10-08T00:38:22.3659482Z","execution_finish_time":"2024-10-08T00:38:22.7055408Z","parent_msg_id":"2ff877cc-1538-4409-b281-4dbcf8b45530"},"text/plain":"StatementMeta(, 2a870d78-2948-4a5b-bc59-32c45802ca9e, 7, Finished, Available, Finished)"},"metadata":{}}],"execution_count":5,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4953148a-93da-4b30-b4bb-ae5f32440236"},{"cell_type":"code","source":["import requests\n","from azure.identity import ClientSecretCredential, AuthenticationRequiredError\n","\n","def get_access_token(app_id, client_secret, directory_id):\n","    try:\n","        # Create the ClientSecretCredential using the provided credentials\n","        # How and where did I find the blog post to enable the App Registration with the valid permissions        \n","\n","        ######################################################################################### \n","        # Read secretes from Azure Key Vault\n","        #########################################################################################\n","        ## This is the name of my Azure Key Vault \n","        key_vault = \"https://domain-keyvault.vault.azure.net/\"\n","        ## I have stored my tenant id as one of the secrets to make it easier to use when needed \n","        tenant = mssparkutils.credentials.getSecret(key_vault , \"tenantid\") \n","        ## This is my application Id for my service principal account \n","        client = mssparkutils.credentials.getSecret(key_vault , \"pbi-applicationid\") \n","        ## This is my Client Secret for my service principal account \n","        clientsecret = mssparkutils.credentials.getSecret(key_vault , \"powerbi-clientsecret\") \n","\n","        # I need to show in the permissions where I selected the Azure Storage.\n","        credential = ClientSecretCredential(\n","            client_id= client,\n","            client_secret= clientsecret,\n","            tenant_id= tenant\n","        ) \n","\n","        # Use the credential to get the access token\n","        token = credential.get_token(\"https://storage.azure.com/.default\").token\n","\n","        return token, credential\n","\n","    except AuthenticationRequiredError as e:\n","        print(\"Authentication failed. Please check your credentials.\")\n","        raise e\n","\n","    except Exception as e:\n","        print(\"An error occurred while getting the access token:\")\n","        print(str(e))\n","        raise e\n","    \n","access_token, credential = get_access_token(\"appId\", \"secret\", \"tenantId\")\n","\n","\n","def check_connection_with_onelake(access_token):\n","    base_url = \"https://onelake.dfs.fabric.microsoft.com/WORKSPACE_GUID/LAKEHOUSE_GUID/Files/\" \n","    token_headers = {\n","        \"Authorization\": \"Bearer \" + access_token\n","    }\n","\n","    try:\n","        response = requests.get(base_url, headers=token_headers)\n","\n","        if response.status_code == 200:\n","            print(\"Connection with OneLake is successful.\")\n","        else:\n","            print(\"Failed to connect with OneLake. Status code:\", response.status_code)\n","            print(response.content)\n","\n","    except requests.exceptions.RequestException as e:\n","        print(\"An error occurred while checking the connection:\", str(e))\n","\n","# Assuming 'access_token' is already defined and contains a valid access token\n","check_connection_with_onelake(access_token)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":8,"statement_ids":[8],"state":"finished","livy_statement_state":"available","session_id":"2a870d78-2948-4a5b-bc59-32c45802ca9e","normalized_state":"finished","queued_time":"2024-10-08T00:38:18.4964226Z","session_start_time":null,"execution_start_time":"2024-10-08T00:38:23.2744792Z","execution_finish_time":"2024-10-08T00:38:24.8650915Z","parent_msg_id":"6e36cfed-179c-4dff-a154-1d6a08df39b4"},"text/plain":"StatementMeta(, 2a870d78-2948-4a5b-bc59-32c45802ca9e, 8, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Connection with OneLake is successful.\n"]}],"execution_count":6,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ba71ce32-2b71-4e28-a946-f43fefd3bfe4"},{"cell_type":"code","source":["# Loop Through Data for Files\n","import datetime\n","from pyspark.sql.types import IntegerType,BooleanType,DateType\n","from pyspark.sql.functions import col, year, month, quarter\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import *\n","from pyspark.sql.functions import lit\n","import requests\n","import json\n","from pandas import json_normalize\n","from pyspark.sql import types as T \n","from pyspark.sql import SparkSession \n","\n","# Create a Spark session\n","sparkdf = spark.createDataFrame(df_Lakehouses_clean)\n","spark = SparkSession.builder.appName(\"AddColumnExample\").getOrCreate()\n","data_collect = sparkdf.collect()\n","\n","# looping thorough each row of the dataframe\n","\n","for row in data_collect:\n","    # Variables Below\n","    var_lakehouseId = row[\"id\"]\n","    var_lakehouseName = row[\"name\"]\n","    var_workspaceId = row[\"workspaceId\"]\n","\n","    try:\n","        \n","        # Get Files Details\n","        mssparkutils.credentials.getToken('storage')\n","        get_url = 'https://onelake.dfs.fabric.microsoft.com/'+ var_workspaceId +'/'+ var_lakehouseId +'/Files?recursive=True&resource=filesystem'\n","        # display(get_url)\n","        resp = requests.get(get_url, headers={\"authorization\":f\"bearer {mssparkutils.credentials.getToken('storage')}\"})\n","        mydata = json.loads(resp.text)\n","\n","        # Convert to Data Frame\n","        pandas_df = json_normalize(mydata, 'paths')\n","\n","        # Reference for DateTime Formats https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior\n","        pandas_df['lastModified'] = pd.to_datetime(pandas_df.lastModified, format = \"%a, %d %b %Y %H:%M:%S %Z\" )\n","\n","        #Convert to Spark Data Frame \n","        df=spark.createDataFrame(pandas_df) \n","\n","        #Additional Columns are added \n","        df = df.withColumn(\"LakehouseId\" , lit(var_lakehouseId)) \n","        df = df.withColumn(\"LakehouseName\" , lit(var_lakehouseName)) \n","        df = df.withColumn(\"WorkspaceId\" , lit(var_workspaceId)) \n","        df = df.withColumn('contentLength', df['contentLength'].cast(T.LongType())) \n","        df = df.withColumn('lastModified', df['lastModified'].cast(T.TimestampType()))\n","        df = df.withColumn(\"creationTime\", from_unixtime(df.creationTime / 1e9).cast(T.TimestampType()))\n","        df = df.withColumn(\"CurrentDateTime\", lit(current_timestamp()))\n","        df = df.withColumn('Year', year(current_timestamp()))\n","        df = df.withColumn('Month', month(current_timestamp()))\n","        df = df.withColumn('Day', dayofmonth(current_timestamp()))\n","                       \n","        #Writing to Lake House\n","        df.write.mode(\"append\").format(\"delta\").partitionBy(\"Year\",\"Month\",\"Day\").save(\"Tables/Onelake_Storage_Files\") \n","\n","    except Exception as e:\n","        Error: {e}"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":12,"statement_ids":[12],"state":"finished","livy_statement_state":"available","session_id":"2a870d78-2948-4a5b-bc59-32c45802ca9e","normalized_state":"finished","queued_time":"2024-10-08T00:38:18.703776Z","session_start_time":null,"execution_start_time":"2024-10-08T00:38:27.7123193Z","execution_finish_time":"2024-10-08T00:38:46.4355813Z","parent_msg_id":"0bcf5583-8af0-40fe-97cb-619042ccbd6a"},"text/plain":"StatementMeta(, 2a870d78-2948-4a5b-bc59-32c45802ca9e, 12, Finished, Available, Finished)"},"metadata":{}}],"execution_count":10,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false,"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"836dde9c-132f-42a1-9b3b-796e03c92547"},{"cell_type":"code","source":["# Loop Through Data for Tables\n","import datetime\n","from pyspark.sql.types import IntegerType,BooleanType,DateType\n","from pyspark.sql.functions import col, year, month, quarter\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import *\n","from pyspark.sql.functions import lit\n","import requests\n","import json\n","from pandas import json_normalize\n","from pyspark.sql import types as T \n","from pyspark.sql import SparkSession \n","\n","# Create a Spark session\n","sparkdf = spark.createDataFrame(df_Lakehouses_clean)\n","spark = SparkSession.builder.appName(\"AddColumnExample\").getOrCreate()\n","data_collect = sparkdf.collect()\n","\n","# looping thorough each row of the dataframe\n","\n","for row in data_collect:\n","#\n","\n","    var_lakehouseId = row[\"id\"]\n","    var_lakehouseName = row[\"name\"]\n","    var_workspaceId = row[\"workspaceId\"]\n","\n","    try:\n","        \n","        # Reference https://www.reddit.com/r/MicrosoftFabric/comments/1bnaz0a/api_to_fetch_list_of_files_in_a_onelake_folder/\n","        mssparkutils.credentials.getToken('storage')\n","        get_url = 'https://onelake.dfs.fabric.microsoft.com/'+ var_workspaceId +'/'+ var_lakehouseId +'/Tables?recursive=True&resource=filesystem'\n","        # display(get_url)\n","        resp = requests.get(get_url, headers={\"authorization\":f\"bearer {mssparkutils.credentials.getToken('storage')}\"})\n","        mydata = json.loads(resp.text)\n","\n","        # Convert to Data Frame\n","        pandas_df = json_normalize(mydata, 'paths')\n","\n","        # Reference for DateTime Formats https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior\n","        pandas_df['lastModified'] = pd.to_datetime(pandas_df.lastModified, format = \"%a, %d %b %Y %H:%M:%S %Z\" )\n","\n","        #Convert to Spark Data Frame \n","\n","        df=spark.createDataFrame(pandas_df) \n","\n","        #Additional Columns are added \n","        df = df.withColumn(\"LakehouseId\" , lit(var_lakehouseId)) \n","        df = df.withColumn(\"LakehouseName\" , lit(var_lakehouseName)) \n","        df = df.withColumn(\"WorkspaceId\" , lit(var_workspaceId)) \n","        \n","\n","        df = df.withColumn('contentLength', df['contentLength'].cast(T.LongType())) \n","        df = df.withColumn('lastModified', df['lastModified'].cast(T.TimestampType()))\n","        df = df.withColumn(\"creationTime\", from_unixtime(df.creationTime / 1e9).cast(T.TimestampType()))\n","        df = df.withColumn(\"CurrentDateTime\", lit(current_timestamp()))\n","        df = df.withColumn('Year', year(current_timestamp()))\n","        df = df.withColumn('Month', month(current_timestamp()))\n","        df = df.withColumn('Day', dayofmonth(current_timestamp()))\n","        # Bulk Rename column Names replacing \"]\" with \"\"\n","               \n","        #Writing to Lake House\n","        df.write.mode(\"append\").format(\"delta\").partitionBy(\"Year\",\"Month\",\"Day\").save(\"Tables/Onelake_Storage_Tables\") \n","\n","    except Exception as e:\n","        Error: {e}"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":13,"statement_ids":[13],"state":"finished","livy_statement_state":"available","session_id":"2a870d78-2948-4a5b-bc59-32c45802ca9e","normalized_state":"finished","queued_time":"2024-10-08T00:38:18.7455475Z","session_start_time":null,"execution_start_time":"2024-10-08T00:38:47.0710834Z","execution_finish_time":"2024-10-08T00:38:59.1546994Z","parent_msg_id":"7e68b16e-3012-44a2-bdf2-18d1e86e74d1"},"text/plain":"StatementMeta(, 2a870d78-2948-4a5b-bc59-32c45802ca9e, 13, Finished, Available, Finished)"},"metadata":{}}],"execution_count":11,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9fd7ad85-1fea-47fc-9bcb-9fbfa2082503"},{"cell_type":"code","source":["# Loop Through Data for Warehouses\n","import datetime\n","from pyspark.sql.types import IntegerType,BooleanType,DateType\n","from pyspark.sql.functions import col, year, month, quarter\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import *\n","from pyspark.sql.functions import lit\n","import requests\n","import json\n","from pandas import json_normalize\n","from pyspark.sql import types as T \n","from pyspark.sql import SparkSession \n","\n","# Create a Spark session\n","sparkdf = spark.createDataFrame(df_Warehouses_clean)\n","spark = SparkSession.builder.appName(\"AddColumnExample\").getOrCreate()\n","data_collect = sparkdf.collect()\n","\n","# looping thorough each row of the dataframe\n","\n","for row in data_collect:\n","#\n","\n","    var_lakehouseId = row[\"id\"]\n","    var_lakehouseName = row[\"name\"]\n","    var_workspaceId = row[\"workspaceId\"]\n","\n","    try:\n","        \n","        # Reference https://www.reddit.com/r/MicrosoftFabric/comments/1bnaz0a/api_to_fetch_list_of_files_in_a_onelake_folder/\n","        mssparkutils.credentials.getToken('storage')\n","        get_url = 'https://onelake.dfs.fabric.microsoft.com/'+ var_workspaceId +'/'+ var_lakehouseId +'/Tables?recursive=True&resource=filesystem'\n","        # display(get_url)\n","        resp = requests.get(get_url, headers={\"authorization\":f\"bearer {mssparkutils.credentials.getToken('storage')}\"})\n","        mydata = json.loads(resp.text)\n","\n","        # Convert to Data Frame\n","        pandas_df = json_normalize(mydata, 'paths')\n","\n","        # Reference for DateTime Formats https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior\n","        pandas_df['lastModified'] = pd.to_datetime(pandas_df.lastModified, format = \"%a, %d %b %Y %H:%M:%S %Z\" )\n","\n","        #Convert to Spark Data Frame \n","\n","        df=spark.createDataFrame(pandas_df) \n","\n","        #Additional Columns are added \n","        df = df.withColumn(\"LakehouseId\" , lit(var_lakehouseId)) \n","        df = df.withColumn(\"LakehouseName\" , lit(var_lakehouseName)) \n","        df = df.withColumn(\"WorkspaceId\" , lit(var_workspaceId)) \n","        \n","\n","        df = df.withColumn('contentLength', df['contentLength'].cast(T.LongType())) \n","        df = df.withColumn('lastModified', df['lastModified'].cast(T.TimestampType()))\n","        df = df.withColumn(\"creationTime\", from_unixtime(df.creationTime / 1e9).cast(T.TimestampType()))\n","        df = df.withColumn(\"CurrentDateTime\", lit(current_timestamp()))\n","        df = df.withColumn('Year', year(current_timestamp()))\n","        df = df.withColumn('Month', month(current_timestamp()))\n","        df = df.withColumn('Day', dayofmonth(current_timestamp()))\n","        # Bulk Rename column Names replacing \"]\" with \"\"\n","               \n","        #Writing to Lake House\n","        df.write.mode(\"append\").format(\"delta\").partitionBy(\"Year\",\"Month\",\"Day\").save(\"Tables/Onelake_Storage_Warehouses\") \n","\n","    except Exception as e:\n","        Error: {e}"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":14,"statement_ids":[14],"state":"finished","livy_statement_state":"available","session_id":"2a870d78-2948-4a5b-bc59-32c45802ca9e","normalized_state":"finished","queued_time":"2024-10-08T00:38:18.7905147Z","session_start_time":null,"execution_start_time":"2024-10-08T00:38:59.7392775Z","execution_finish_time":"2024-10-08T00:39:06.0184736Z","parent_msg_id":"03834306-f548-4c68-ae92-c1d828c9e8ce"},"text/plain":"StatementMeta(, 2a870d78-2948-4a5b-bc59-32c45802ca9e, 14, Finished, Available, Finished)"},"metadata":{}}],"execution_count":12,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ce05ab75-f2c0-41ca-bf64-727a675c8159"}],"metadata":{"language_info":{"name":"python"},"kernel_info":{"name":"synapse_pyspark"},"microsoft":{"language":"python","ms_spell_check":{"ms_spell_check_language":"en"},"language_group":"synapse_pyspark"},"widgets":{},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default"},"dependencies":{"lakehouse":{"default_lakehouse":"260cc545-9a59-45a7-9897-fb705d4bc4ae","default_lakehouse_name":"FM_LH","default_lakehouse_workspace_id":"cb2af739-998e-45c8-8c41-f78d2e8fc1dd","known_lakehouses":[{"id":"260cc545-9a59-45a7-9897-fb705d4bc4ae"}]}}},"nbformat":4,"nbformat_minor":5}