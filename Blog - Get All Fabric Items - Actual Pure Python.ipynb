{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13ff9c13-6998-43fd-938e-5df0cd7f5aea",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": "2025-03-04T06:52:44.0609671Z",
       "execution_start_time": "2025-03-04T06:52:43.5992088Z",
       "normalized_state": "finished",
       "parent_msg_id": "3176eadd-d2f1-40dc-b4f9-55779f93ee0b",
       "queued_time": "2025-03-04T06:52:39.9945194Z",
       "session_id": "285ec360-440e-4231-a165-12e8c463f47d",
       "session_start_time": "2025-03-04T06:52:39.9957809Z"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import notebookutils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde0cb3e-ad75-4304-8f7a-301412d0d10f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "######################################################################################### \n",
    "# Read secretes from Azure Key Vault\n",
    "#########################################################################################\n",
    "## This is the name of my Azure Key Vault \n",
    "key_vault = \"https://company-keyvault.vault.azure.net/\"\n",
    "## I have stored my tenant id as one of the secrets to make it easier to use when needed \n",
    "tenant = notebookutils.credentials.getSecret(key_vault , \"tenantid\") \n",
    "## This is my application Id for my service principal account \n",
    "client = notebookutils.credentials.getSecret(key_vault , \"powerbi-applicationid\") \n",
    "## This is my Client Secret for my service principal account \n",
    "client_secret = notebookutils.credentials.getSecret(key_vault , \"powerbi-clientsecret\")  \n",
    "\n",
    "######################################################################################### \n",
    "# Authentication - Replace string variables with your relevant values \n",
    "#########################################################################################  \n",
    "\n",
    "import json, requests, pandas as pd \n",
    "import datetime  \n",
    "\n",
    "try: \n",
    "    from azure.identity import ClientSecretCredential \n",
    "except Exception:\n",
    "     !pip install azure.identity \n",
    "     from azure.identity import ClientSecretCredential \n",
    "\n",
    "# Generates the access token for the Service Principal \n",
    "api = 'https://analysis.windows.net/powerbi/api/.default' \n",
    "auth = ClientSecretCredential(authority = 'https://login.microsoftonline.com/', \n",
    "               tenant_id = tenant, \n",
    "               client_id = client, \n",
    "               client_secret = client_secret) \n",
    "access_token = auth.get_token(api)\n",
    "access_token = access_token.token \n",
    "\n",
    "## This is where I store my header with the Access Token, because this is required when authenticating \n",
    "## to the Power BI Admin APIs \n",
    "header = {'Authorization': f'Bearer {access_token}'}  \n",
    "\n",
    "print('\\nSuccessfully authenticated.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5da554e-331a-44c9-a060-33a515adfbd9",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# # Get all Users and download into JSON File\n",
    "# # If needed to get more details here is how: https://stackoverflow.com/questions/48229949/get-all-user-properties-from-microsoft-graph\n",
    "# # Here is an example for the API to get all details: https://graph.microsoft.com/v1.0/groups?$filter=displayname eq 'sec_Office365_FabricAdmin_Role'&$expand=members\n",
    "\n",
    "import requests\n",
    "import msal\n",
    "import datetime\n",
    "from datetime import datetime,date,timedelta\n",
    "\n",
    "# Microsoft Graph API endpoint for listing users\n",
    "GRAPH_API_URL = 'https://api.fabric.microsoft.com/v1/admin/items'\n",
    "\n",
    "\n",
    "# Define headers for API request\n",
    "headers = {\n",
    "    'Authorization': 'Bearer ' + access_token,\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "# Make the API request to list users\n",
    "response = requests.get(GRAPH_API_URL, headers=headers)\n",
    "# data = response.json()\n",
    "# display(data)\n",
    "\n",
    "# To hold all users\n",
    "all_users = []\n",
    "\n",
    "# Pagination loop\n",
    "while GRAPH_API_URL:\n",
    "    response = requests.get(GRAPH_API_URL, headers=headers)\n",
    "    data = response.json()\n",
    "    \n",
    "    # Process users\n",
    "    all_users.extend(data['itemEntities'])\n",
    "    \n",
    "    # Get the next page URL\n",
    "    GRAPH_API_URL = data.get('@odata.nextLink')\n",
    "\n",
    "# Create Directory if it does not exist\n",
    "notebookutils.fs.mkdirs(\"Files/Fabric_Items/\")\n",
    "\n",
    "fileName = 'Fabric_Items_' + (datetime.today()).strftime('%Y%m%d') + '.json'\n",
    "\n",
    "# Write the output to a JSON file\n",
    "with open(f\"/lakehouse/default/Files/Fabric_Items/{fileName}\", \"w\") as json_file:\n",
    "    json.dump(all_users, json_file, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1aff0a3-318a-4998-8410-ed13e94e2bc5",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# # Getting the JSON Users File into Dataframe\n",
    "# REFERENCE: https://medium.com/@mariusz_kujawski/python-in-microsoft-fabric-data-factory-vs-polars-and-duckdb-06dc4383475b\n",
    "import duckdb\n",
    "from deltalake import write_deltalake, DeltaTable\n",
    "import pyarrow\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "fileName = 'Fabric_Items_' + (datetime.today()).strftime('%Y%m%d') + '.json'\n",
    "\n",
    "with open(f'/lakehouse/default/Files/Fabric_Items/{fileName}') as f:\n",
    "#with open(df, encoding=\"utf-16\") as f:\n",
    "   data = json.load(f)\n",
    "\n",
    "# Get the Expanded Members, by using the json normalize and then have the higher levels of the json structure.\n",
    "df = pd.json_normalize(data)\n",
    "\n",
    "storage_options = {\"use_fabric_endpoint\": \"true\", \"allow_unsafe_rename\":\"true\", \"bearer_token\": notebookutils.credentials.getToken('storage')}\n",
    "\n",
    "\n",
    "path_table = f\"/lakehouse/default/Tables/staging_all_fabric_items\"\n",
    "write_deltalake(path_table, df, mode=\"overwrite\", engine='rust', storage_options=storage_options)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfebf68f-9da3-4be6-9bc7-0b747ef29c75",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "import duckdb\n",
    "from deltalake import DeltaTable, write_deltalake\n",
    "\n",
    "# Connect to DuckDB\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Install the delta extension\n",
    "con.execute(\"INSTALL delta;\")\n",
    "\n",
    "# Load the delta extension\n",
    "con.execute(\"LOAD delta;\")\n",
    "\n",
    "# Load source and target as Delta tables (replace paths with your lakehouse paths)\n",
    "source_path = \"/lakehouse/default/Tables/staging_all_fabric_items\"\n",
    "target_path = \"/lakehouse/default/Tables/all_fabric_items\"\n",
    "\n",
    "# Register Delta tables\n",
    "con.execute(f\"CREATE TABLE target_table AS SELECT * FROM delta_scan('{target_path}')\")\n",
    "con.execute(f\"CREATE TABLE source_table AS SELECT * FROM delta_scan('{source_path}')\")\n",
    "# con.execute(f\"CREATE TABLE final_table AS SELECT * FROM delta_scan('{target_path}')\")\n",
    "\n",
    "# Perform the merge (same as above)\n",
    "con.execute(\"\"\"\n",
    "    UPDATE target_table AS t\n",
    "    SET name = s.name,\n",
    "        type = s.type\n",
    "    FROM source_table AS s\n",
    "    WHERE t.id = s.id\n",
    "\"\"\")\n",
    "con.execute(\"\"\"\n",
    "    INSERT INTO target_table\n",
    "    SELECT s.*\n",
    "    FROM source_table AS s\n",
    "    LEFT JOIN target_table AS t ON s.id = t.id\n",
    "    WHERE t.id IS NULL\n",
    "\"\"\")\n",
    "\n",
    "# Verify and create a dataframe with the resulting target table.\n",
    "result = con.execute(\"SELECT * FROM target_table\").fetchdf()\n",
    "# result = con.execute(\"SELECT s.* FROM target_table as s LEFT JOIN final_table AS t ON s.id = t.id WHERE t.id IS NULL\").fetchdf()\n",
    "display(result)\n",
    "\n",
    "# Execute and fetch results\n",
    "try:\n",
    "    # Write back to Delta (replace target_path with your actual path)\n",
    "    # Reference: https://datamonkeysite.com/2023/11/01/loading-delta-table-to-fabric-onelake-using-delta-rust/\n",
    "    write_deltalake(f\"/lakehouse/default/Tables/all_fabric_items_New\",result,engine='rust',mode=\"overwrite\",  storage_options={\"allow_unsafe_rename\":\"true\"})\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "environment": {},
   "lakehouse": {
    "default_lakehouse": "260cc545-9a59-45a7-9897-fb705d4bc4ae",
    "default_lakehouse_name": "FM_LH",
    "default_lakehouse_workspace_id": "cb2af739-998e-45c8-8c41-f78d2e8fc1dd"
   }
  },
  "kernel_info": {
   "jupyter_kernel_name": "python3.11",
   "name": "jupyter"
  },
  "kernelspec": {
   "display_name": "Jupyter",
   "language": "Jupyter",
   "name": "jupyter"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "jupyter_python",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  },
  "widgets": {}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
