{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9e8e97e-7191-43ab-a132-828a9d35da0b",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "\n",
    "# Microsoft Fabric - Bulk Export All Items to Local Files using Fabric CLI (Python)\n",
    "\n",
    "This notebook uses the official Microsoft Fabric CLI (`fab`) from Python to export every exportable item from one or more workspaces.\n",
    "- Idea for this based off the blog post: https://peerinsights.emono.dk/fabric-cli-beyond-shell-commands?s=03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa9e27d-a46c-48c6-a77d-b525b8fed24f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": "2025-11-25T06:28:23.9540464Z",
       "execution_start_time": "2025-11-25T06:28:22.5695265Z",
       "normalized_state": "finished",
       "parent_msg_id": "f534aa57-8784-4f66-b40e-2bcd2764b201",
       "queued_time": "2025-11-25T06:28:20.5540215Z",
       "session_id": "e1df8fa2-5c03-4cb1-aa81-ca4b11f710ee",
       "session_start_time": "2025-11-25T06:28:20.5548656Z"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#########################\n",
    "## Input Parameters\n",
    "#########################\n",
    "\n",
    "# This is the Workspace Name for the items you want to download - NOTE: It has got the .Workspace at the end as this is required for the Fabric CLI\n",
    "target_workspaceName = \"FILL ME IN.Workspace\"\n",
    "\n",
    "# This is the Workspace GUID\n",
    "target_workspace_id = \"FILL ME IN\" \n",
    "\n",
    "# Lakehouse Name\n",
    "target_lakehouse = \"FILL ME IN\"          \n",
    "\n",
    "# Lakehouse GUID\n",
    "target_lakehouse_id = \"FILL ME IN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93c4d97c-e0f5-4118-b3e0-38845f74bf37",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": "2025-11-25T06:28:32.6394213Z",
       "execution_start_time": "2025-11-25T06:28:23.9553927Z",
       "normalized_state": "finished",
       "parent_msg_id": "3b2189f0-a282-40bb-88ef-e02624bb7343",
       "queued_time": "2025-11-25T06:28:20.5556253Z",
       "session_id": "e1df8fa2-5c03-4cb1-aa81-ca4b11f710ee",
       "session_start_time": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fabric CLI installed/updated\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Install Fabric CLI (only needed once per environment)\n",
    "import sys\n",
    "!{sys.executable} -m pip install --quiet ms-fabric-cli --upgrade\n",
    "print(\"Fabric CLI installed/updated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75ecba0-6558-40e6-be12-5657042f1f7e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": "2025-11-25T06:28:39.7226182Z",
       "execution_start_time": "2025-11-25T06:28:32.6406472Z",
       "normalized_state": "finished",
       "parent_msg_id": "587c63a7-d377-4fae-900d-c1c338ae6d89",
       "queued_time": "2025-11-25T06:28:20.5571016Z",
       "session_id": "e1df8fa2-5c03-4cb1-aa81-ca4b11f710ee",
       "session_start_time": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully authenticated.\n"
     ]
    }
   ],
   "source": [
    "######################################################################################### \n",
    "# Read secretes from Azure Key Vault\n",
    "#########################################################################################\n",
    "## This is the name of my Azure Key Vault \n",
    "key_vault = \"https://FILL ME IN.vault.azure.net/\"\n",
    "## I have stored my tenant id as one of the secrets to make it easier to use when needed \n",
    "tenant = notebookutils.credentials.getSecret(key_vault , \"FILL ME IN\") \n",
    "## This is my application Id for my service principal account \n",
    "client = notebookutils.credentials.getSecret(key_vault , \"FILL ME IN\") \n",
    "## This is my Client Secret for my service principal account \n",
    "client_secret = notebookutils.credentials.getSecret(key_vault , \"FILL ME IN\")\n",
    "\n",
    "\n",
    "######################################################################################### \n",
    "# Authentication - Replace string variables with your relevant values \n",
    "#########################################################################################  \n",
    "\n",
    "import json, requests, pandas as pd \n",
    "import datetime  \n",
    "\n",
    "try: \n",
    "    from azure.identity import ClientSecretCredential \n",
    "except Exception:\n",
    "     !pip install azure.identity \n",
    "     from azure.identity import ClientSecretCredential \n",
    "\n",
    "# Generates the access token for the Service Principal \n",
    "api = 'https://analysis.windows.net/powerbi/api/.default' \n",
    "auth = ClientSecretCredential(authority = 'https://login.microsoftonline.com/', \n",
    "               tenant_id = tenant, \n",
    "               client_id = client, \n",
    "               client_secret = client_secret) \n",
    "access_token = auth.get_token(api)\n",
    "access_token = access_token.token \n",
    "\n",
    "## This is where I store my header with the Access Token, because this is required when authenticating \n",
    "## to the Power BI Admin APIs \n",
    "header = {'Authorization': f'Bearer {access_token}'}  \n",
    "\n",
    "print('\\nSuccessfully authenticated.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b3da15",
   "metadata": {},
   "source": [
    "## Begin Login to Fabric CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1f51410-8129-4793-89fc-89520ea290c4",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": "2025-11-25T06:28:41.0335556Z",
       "execution_start_time": "2025-11-25T06:28:39.723924Z",
       "normalized_state": "finished",
       "parent_msg_id": "9c07e954-7447-42a7-a78b-ceefebcc981d",
       "queued_time": "2025-11-25T06:28:20.5586468Z",
       "session_id": "e1df8fa2-5c03-4cb1-aa81-ca4b11f710ee",
       "session_start_time": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?7h\u001b[0;38;5;244mUpdating 'encryption_fallback_enabled' value...\u001b[0m\r\r\n",
      "\u001b[0m\u001b[0m\u001b[?7h\u001b[0;32m*\u001b[0m Configuration 'encryption_fallback_enabled' set to 'true'\r\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!fab config set encryption_fallback_enabled true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9ce3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from argparse import Namespace\n",
    "from fabric_cli.commands.config import fab_config\n",
    "from fabric_cli.commands.auth import fab_auth\n",
    "\n",
    "# Login using service principal\n",
    "args = Namespace(\n",
    "    auth_command=\"login\",\n",
    "    username=client,\n",
    "    password=client_secret,\n",
    "    tenant=tenant,\n",
    "    identity=None,\n",
    "    federated_token=None,\n",
    "    certificate=None\n",
    ")\n",
    "fab_auth.init(args)\n",
    "fab_auth.status(None)  # Check current authentication status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0e7348d-d1af-4e28-9be8-52d8ef28d90f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": "2025-11-25T06:28:43.3986693Z",
       "execution_start_time": "2025-11-25T06:28:43.06385Z",
       "normalized_state": "finished",
       "parent_msg_id": "6b7b93bb-352e-48a5-abe3-4af3c4aaa609",
       "queued_time": "2025-11-25T06:28:20.5616882Z",
       "session_id": "e1df8fa2-5c03-4cb1-aa81-ca4b11f710ee",
       "session_start_time": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 3: List all your workspaces (helps you pick the ones you want)\n",
    "# !fab ls /Fabric_FourMoo.Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4763cafd",
   "metadata": {},
   "source": [
    "## Get all Items and create a Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a7d17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import subprocess\n",
    "\n",
    "# Capture the command output\n",
    "result = subprocess.run(\n",
    "    ['fab', 'ls', f\"/{target_workspaceName}\"],\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "# Get the output\n",
    "output = result.stdout\n",
    "\n",
    "# Parse the ls -la output\n",
    "lines = output.strip().split('\\n')\n",
    "\n",
    "# Skip the first line (total) if present\n",
    "if lines[0].startswith('total'):\n",
    "    lines = lines[1:]\n",
    "\n",
    "data = []\n",
    "for line in lines:\n",
    "        data.append({\n",
    "            'item_name': line\n",
    "        })\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b48e34fd-907c-4dac-be56-43f0dcb77826",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": "2025-11-25T06:28:47.8854551Z",
       "execution_start_time": "2025-11-25T06:28:45.4733617Z",
       "normalized_state": "finished",
       "parent_msg_id": "e54ef026-bb93-4eee-aba2-c3d941089dfd",
       "queued_time": "2025-11-25T06:28:20.5649404Z",
       "session_id": "e1df8fa2-5c03-4cb1-aa81-ca4b11f710ee",
       "session_start_time": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.synapse-jupyter.display-view+json": {
       "isSummary": false,
       "language": "python",
       "table": {
        "rows": [
         {
          "0": "A.RunPerfScenario.Notebook"
         },
         {
          "0": "Adding Service Principal to all App Workspaces.Notebook"
         },
         {
          "0": "Assign to Fabric F2 Capacity.Notebook"
         },
         {
          "0": "Azure Key Vault Auth with Service Principal.Notebook"
         },
         {
          "0": "B.RunLoadTest.Notebook"
         },
         {
          "0": "BLOG - Daily Update Incremental Refresh Policy Multiple Facts.Notebook"
         },
         {
          "0": "Blog - DuckDB SQL Code to read from Delta Tables.Notebook"
         },
         {
          "0": "BLOG - Entra ID All Group Members.Notebook"
         },
         {
          "0": "Blog - Export all Items to OneLake.Notebook"
         },
         {
          "0": "Blog - Get All Entra ID Groups and Users and Licenses.Notebook"
         },
         {
          "0": "Blog - Get All Fabric Items - Actual Pure Python.Notebook"
         },
         {
          "0": "Blog - Get All Fabric Items - Pure Python.Notebook"
         },
         {
          "0": "Blog - Get All Fabric Items - PySpark.Notebook"
         },
         {
          "0": "Blog - Get All Fabric Items - Spark.Notebook"
         },
         {
          "0": "Blog - Get Dataset ID DataSourceID ConnectionTypes and Gateway Stuff.Notebook"
         },
         {
          "0": "Blog - Get Files and Table Sizes.Notebook"
         },
         {
          "0": "Blog - Get Item Last Run Status.Notebook"
         },
         {
          "0": "Blog - IR Policy Updates.Notebook"
         },
         {
          "0": "Blog - Looping Through Data.Notebook"
         },
         {
          "0": "Blog - Optimize Lakehouse Tables.Notebook"
         },
         {
          "0": "Blog - PartitionBy.Notebook"
         },
         {
          "0": "Blog - Python - DuckDB - Looping and write once.Notebook"
         },
         {
          "0": "Blog - Python - DuckDB - Querying Multiple Tables.Notebook"
         },
         {
          "0": "Blog - Python - DuckDB - Writing to Lakehouse Table.Notebook"
         },
         {
          "0": "Blog - Python - Export SM to BIM.Notebook"
         },
         {
          "0": "Blog - Python - Extract all Metadata from Semantic Model.Notebook"
         },
         {
          "0": "Blog - Python - Loop DAX Query and write to LH Table.Notebook"
         },
         {
          "0": "Blog - Python - Run DAX Query and write to LH Table.Notebook"
         },
         {
          "0": "Blog - Reading and Write different Lakehouses.Notebook"
         },
         {
          "0": "Blog - Saving API Response to Dataframe.Notebook"
         },
         {
          "0": "BLOG - Show Table with IR Policy.Notebook"
         },
         {
          "0": "Blog - SRG Example - Get All Entra ID Groups and Users.Notebook"
         },
         {
          "0": "Blog - Update IR Policy.Notebook"
         },
         {
          "0": "Blog - Update IR Policy $ Script out DB.Notebook"
         },
         {
          "0": "BPA Rules.Notebook"
         },
         {
          "0": "BPA_Monitoring.Notebook"
         },
         {
          "0": "Capacity Metrics.Notebook"
         },
         {
          "0": "Convert from Import to DirectLake.Notebook"
         },
         {
          "0": "Convert from TSQL to SparkSQL.Notebook"
         },
         {
          "0": "Copy Another Tenant SQL.DataPipeline"
         },
         {
          "0": "Copy from users My Worksapce to Workspace.Notebook"
         },
         {
          "0": "copy_wwi_DirectLake.CopyJob"
         },
         {
          "0": "Create case insensitive Warehouse.Notebook"
         },
         {
          "0": "Create Warehouse with Service Principal.Notebook"
         },
         {
          "0": "Creating WH Schema from DataMart.Notebook"
         },
         {
          "0": "Daily Update Incremental Refresh Policy.Notebook"
         },
         {
          "0": "DataflowsStagingLakehouse.Lakehouse"
         },
         {
          "0": "DataflowsStagingLakehouse.SemanticModel"
         },
         {
          "0": "DataflowsStagingLakehouse.SQLEndpoint"
         },
         {
          "0": "DataflowsStagingWarehouse.SemanticModel"
         },
         {
          "0": "DataflowsStagingWarehouse.Warehouse"
         },
         {
          "0": "DF_LoadingTestFiles.Dataflow"
         },
         {
          "0": "Direct Lake Report.Report"
         },
         {
          "0": "DL - Near Real Time Report.Report"
         },
         {
          "0": "EH_Test.Eventhouse"
         },
         {
          "0": "EH_Test.KQLDatabase"
         },
         {
          "0": "EH_Test_queryset.KQLQueryset"
         },
         {
          "0": "Excel_Mirrored_DB.MirroredDatabase"
         },
         {
          "0": "Excel_Mirrored_DB.SemanticModel"
         },
         {
          "0": "Excel_Mirrored_DB.SQLEndpoint"
         },
         {
          "0": "External Sharing with Data Example.Report"
         },
         {
          "0": "External Sharing with Data Example.SemanticModel"
         },
         {
          "0": "Fabric Metrics Data Behind Alert.Reflex"
         },
         {
          "0": "Fabric Storage Collection.Notebook"
         },
         {
          "0": "FabricMonitoring_TenantSettings_GetData.Notebook"
         },
         {
          "0": "FabricMonitoring_TenantSettings_RefreshSemanticModel.Notebook"
         },
         {
          "0": "FabricMonitoring_TenantSettings_TransformData.Notebook"
         },
         {
          "0": "First TSQL Notebook.Notebook"
         },
         {
          "0": "FM Org App.OrgApp"
         },
         {
          "0": "FM-Fabric Capacity Metrics Report.Report"
         },
         {
          "0": "FM_LH.Lakehouse"
         },
         {
          "0": "FM_LH.SemanticModel"
         },
         {
          "0": "FM_LH.SQLEndpoint"
         },
         {
          "0": "fn_WriteToLakeHouse.UserDataFunction"
         },
         {
          "0": "FourMoo - Capacity Metrics.Notebook"
         },
         {
          "0": "FourMoo - Capacity Metrics WIP.Notebook"
         },
         {
          "0": "FourMoo - Capacity Metrics With TimePoints.Notebook"
         },
         {
          "0": "FourMoo - Capacity Metrics With TimePoints V3.Notebook"
         },
         {
          "0": "FourMoo - Capacity Metrics With TimePoints WIP2.Notebook"
         },
         {
          "0": "FourMoo - Capacity Metrics-WIP2.Notebook"
         },
         {
          "0": "FourMoo - Semantic Model - Fabric Capacity Metrics.SemanticModel"
         },
         {
          "0": "FourMoo - Testing Capacity Metrics With TimePoints PYTHON ONLY.Notebook"
         },
         {
          "0": "FourMoo Org App.OrgApp"
         },
         {
          "0": "GA - Active Users Report.Report"
         },
         {
          "0": "GA-Active-Users.SemanticModel"
         },
         {
          "0": "Get Entra ID Group Members.Notebook"
         },
         {
          "0": "Get Files and Table Sizes.Notebook"
         },
         {
          "0": "Get table schema.Notebook"
         },
         {
          "0": "Google Analytics Real-time.Notebook"
         },
         {
          "0": "Lakehouse and Warehouse Files and Table Sizes.Notebook"
         },
         {
          "0": "LH_NewSchema.Lakehouse"
         },
         {
          "0": "LH_NewSchema.SemanticModel"
         },
         {
          "0": "LH_NewSchema.SQLEndpoint"
         },
         {
          "0": "Load Azure Costs from CSV.Notebook"
         },
         {
          "0": "Load from Shortcut to Lakehouse.Notebook"
         },
         {
          "0": "Load Notebook and Dataflow Gen2.DataPipeline"
         },
         {
          "0": "Load Storage Used.DataPipeline"
         },
         {
          "0": "Load Test SM.SemanticModel"
         },
         {
          "0": "Load Testing.Report"
         },
         {
          "0": "Load Testing Results.Report"
         },
         {
          "0": "Load Testing Results.SemanticModel"
         },
         {
          "0": "MetricsAppData.SemanticModel"
         },
         {
          "0": "MetricsAppSnapshots_Daily.Notebook"
         },
         {
          "0": "MetricsAppSnapshots_Once.Notebook"
         },
         {
          "0": "Migrate from Import to DirectLake - Different Workspaces.Notebook"
         },
         {
          "0": "Mirrored-FourMoo-DB.MirroredDatabase"
         },
         {
          "0": "Mirrored-FourMoo-DB.SemanticModel"
         },
         {
          "0": "Mirrored-FourMoo-DB.SQLEndpoint"
         },
         {
          "0": "Native_Execution.Environment"
         },
         {
          "0": "NB - Change Ownership.Notebook"
         },
         {
          "0": "NB - Testing External Data Share.Notebook"
         },
         {
          "0": "NB - Upload to S3.Notebook"
         },
         {
          "0": "Near Realtime Semantic Model.SemanticModel"
         },
         {
          "0": "Notebook 2.Notebook"
         },
         {
          "0": "Notebook 229.Notebook"
         },
         {
          "0": "Notebook 231.Notebook"
         },
         {
          "0": "PBI Scanner API.Notebook"
         },
         {
          "0": "PL - Power BI Audit Logs.DataPipeline"
         },
         {
          "0": "PL - Semantic Model Memory Sizes.DataPipeline"
         },
         {
          "0": "PL - SQL On-Premise.DataPipeline"
         },
         {
          "0": "PL - Table Maintenance FM_LH.DataPipeline"
         },
         {
          "0": "PL- Scanner API.DataPipeline"
         },
         {
          "0": "PL_On_Premise_Fact_Sale.Lakehouse"
         },
         {
          "0": "PL_On_Premise_Fact_Sale.SemanticModel"
         },
         {
          "0": "PL_On_Premise_Fact_Sale.SQLEndpoint"
         },
         {
          "0": "Power Automate Get Flow History.Notebook"
         },
         {
          "0": "Power BI Audit Logs.Notebook"
         },
         {
          "0": "PP - Table Maintenance.Notebook"
         },
         {
          "0": "Process IR Tables.Notebook"
         },
         {
          "0": "Pure Python Patterns.Notebook"
         },
         {
          "0": "PurePython - Capacity Metrics Extract.Notebook"
         },
         {
          "0": "PurePython - Capacity Metrics Extract - Aug 2025.Notebook"
         },
         {
          "0": "PurePython - Capacity Metrics Extract_2025-07-01.Notebook"
         },
         {
          "0": "PurePython - Capacity Metrics With Overages V4.Notebook"
         },
         {
          "0": "PurePython - Capacity Metrics With TimePoints V1.Notebook"
         },
         {
          "0": "PurePython - Capacity Metrics With TimePoints V2.Notebook"
         },
         {
          "0": "PurePython - Capacity Metrics With TimePoints V3 Migration.Notebook"
         },
         {
          "0": "Python - Table Maintenance.Notebook"
         },
         {
          "0": "Python Config Settings.Notebook"
         },
         {
          "0": "Python IR Policy Update Prev 6 Months.Notebook"
         },
         {
          "0": "Python Only Notebook Test.Notebook"
         },
         {
          "0": "Reading Table from Another Lakehouse.Notebook"
         },
         {
          "0": "Refresh all Usage Metrics Semantic Models.Notebook"
         },
         {
          "0": "Refresh History - Usage Metrics Reports.Notebook"
         },
         {
          "0": "RefreshSemanticModel1.DataPipeline"
         },
         {
          "0": "Run Metrics App data load daily.DataPipeline"
         },
         {
          "0": "Run Report Usage Metrics.DataPipeline"
         },
         {
          "0": "Run_Load_Testing.DataPipeline"
         },
         {
          "0": "RunLoadTest_WithParameters.Notebook"
         },
         {
          "0": "Sales - Semantic Model.SemanticModel"
         },
         {
          "0": "Sales Org.OrgApp"
         },
         {
          "0": "Scanner API.Notebook"
         },
         {
          "0": "Scanner API extraction (selected workspaces).Notebook"
         },
         {
          "0": "Script out Table.Notebook"
         },
         {
          "0": "Semantic Model Memory Sizes.Notebook"
         },
         {
          "0": "Send to S3.DataPipeline"
         },
         {
          "0": "Sian Pipeline.DataPipeline"
         },
         {
          "0": "SLL - Demo.Notebook"
         },
         {
          "0": "SLL - Intro.Notebook"
         },
         {
          "0": "SLL - Python IR Policy Update Prev 6 Months Copy.Notebook"
         },
         {
          "0": "StagingLakehouseForDataflows_20251022031729.Lakehouse"
         },
         {
          "0": "StagingLakehouseForDataflows_20251022031729.SQLEndpoint"
         },
         {
          "0": "StagingWarehouseForDataflows_20251022031741.Warehouse"
         },
         {
          "0": "Start Pipeline API.Notebook"
         },
         {
          "0": "Table Maintenance.Notebook"
         },
         {
          "0": "Temp - Delete Data.Notebook"
         },
         {
          "0": "tesla_solar_download.Notebook"
         },
         {
          "0": "tesla_solar_download Copy.Notebook"
         },
         {
          "0": "tesla_solar_download Copy1.Notebook"
         },
         {
          "0": "Test Warehouse.SemanticModel"
         },
         {
          "0": "Test Warehouse.Warehouse"
         },
         {
          "0": "TEst1.GraphQLApi"
         },
         {
          "0": "Testing DirectQuery with Big Table.Report"
         },
         {
          "0": "Testing DirectQuery with Big Table.SemanticModel"
         },
         {
          "0": "UM Report.Report"
         },
         {
          "0": "Update IR Policy.DataPipeline"
         },
         {
          "0": "Usage Metrics Extractor - June 2024.Notebook"
         },
         {
          "0": "Usage Metrics Extractor - V2.Notebook"
         },
         {
          "0": "Usage Metrics Report.Report"
         },
         {
          "0": "Usage Metrics Report.SemanticModel"
         },
         {
          "0": "WH_Case_Insensitive_DB.SemanticModel"
         },
         {
          "0": "WH_Case_Insensitive_DB.Warehouse"
         },
         {
          "0": "WH_Case_Insensitive_DB_Test.SemanticModel"
         },
         {
          "0": "WH_Case_Insensitive_DB_Test.Warehouse"
         },
         {
          "0": "WH_Service_Principal.SemanticModel"
         },
         {
          "0": "WH_Service_Principal.Warehouse"
         },
         {
          "0": "World Wide Importers - Power BI - Completed_DL.SemanticModel"
         },
         {
          "0": "Write once.Notebook"
         },
         {
          "0": "Write to Azure Blob Storage.Notebook"
         },
         {
          "0": "WWI Sales - Azure SQL Source - PPU - 48 Months - 2 Days_DL.SemanticModel"
         },
         {
          "0": "WWI Sales - Azure SQL_DL.SemanticModel"
         },
         {
          "0": "WWI_DL.SemanticModel"
         }
        ],
        "schema": [
         {
          "key": "0",
          "name": "item_name",
          "type": "string"
         }
        ],
        "truncated": false
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22985061-8741-420b-951b-54844736478f",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": "2025-11-25T06:05:50.6424303Z",
       "execution_start_time": "2025-11-25T06:05:50.3191347Z",
       "normalized_state": "finished",
       "parent_msg_id": "6d5ed043-4588-4763-929c-986bac750555",
       "queued_time": "2025-11-25T06:05:20.019679Z",
       "session_id": "4e80ef7b-d5b7-44f8-a1ce-774cb5834579",
       "session_start_time": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##################################\n",
    "## Exporting to OneLake\n",
    "##################################\n",
    "\n",
    "\n",
    "# OneLake path template (Files is the default for uploads)\n",
    "onelake_base = f\"abfss://{target_workspace_id}@onelake.dfs.fabric.microsoft.com/{target_lakehouse_id}/Files\"\n",
    "\n",
    "all_usage_dfs = []\n",
    "\n",
    "oneLake_export_folder_name = \"AllItemsExport\"\n",
    "\n",
    "# Loop through each row and collect data\n",
    "for row in df.itertuples():\n",
    "    try:\n",
    "        # Assign variables from row\n",
    "        var_ItemName = str(row.item_name).strip()   # make sure no trailing spaces\n",
    "\n",
    "        print(var_ItemName)\n",
    "    \n",
    "        # ←←← THESE ARE THE ONLY LINES YOU ASKED TO CHANGE\n",
    "        workspace_name = target_workspaceName          # your workspace\n",
    "        item_name      = var_ItemName                        # ← use the name from the row\n",
    "\n",
    "        # Where to save it (local temp → OneLake)\n",
    "        import tempfile, os\n",
    "        from datetime import datetime\n",
    "\n",
    "        with tempfile.TemporaryDirectory() as tmp:\n",
    "            # Create a folder inside the temp directory that matches the item name\n",
    "            item_output_folder = f\"{tmp}/{oneLake_export_folder_name}/{var_ItemName}\"\n",
    "            os.makedirs(item_output_folder, exist_ok=True)\n",
    "            \n",
    "            full_item_path = f\"/{workspace_name}/{item_name}\"\n",
    "            \n",
    "            print(f\"Exporting: {full_item_path}\")\n",
    "            !fab export \"{full_item_path}\" -f -o \"{item_output_folder}\"\n",
    "            \n",
    "            # Copy the whole item folder (not just the AllItemsExport root) to OneLake\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            onelake_path = f\"{onelake_base}/{oneLake_export_folder_name}/{var_ItemName}_{timestamp}\"\n",
    "            print(f\"OneLakePath={onelake_path}\")\n",
    "            \n",
    "            notebookutils.fs.mkdirs(onelake_path)\n",
    "            notebookutils.fs.cp(f\"file:{item_output_folder}\", onelake_path, recurse=True)\n",
    "            \n",
    "            # print(f\"\\nDone! Item exported to OneLake:\")\n",
    "            # displayHTML(f'<h3><a href=\"{onelake_path}\" target=\"_blank\">Open {var_ItemName} in OneLake</a></h3>')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error exporting item '{var_ItemName}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6d2b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #####################################\n",
    "# ## Export to Azure Storage Account\n",
    "# #####################################\n",
    "%pip install azure-identity azure-storage-blob azure-storage-file-datalake --quiet\n",
    "\n",
    "# Import libraries\n",
    "from azure.identity import ClientSecretCredential\n",
    "from azure.storage.filedatalake import DataLakeServiceClient\n",
    "import tempfile\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Storage Details\n",
    "storage_account_name = \"FILL ME IN\"\n",
    "container_name       = \"FILL ME IN\"\n",
    "\n",
    "# Service Principal Credentials (replace or use secrets)\n",
    "TENANT_ID = tenant\n",
    "CLIENT_ID = client\n",
    "CLIENT_SECRET = client_secret\n",
    "\n",
    "# Create credential\n",
    "credential = ClientSecretCredential(\n",
    "    tenant_id=TENANT_ID,\n",
    "    client_id=CLIENT_ID,\n",
    "    client_secret=CLIENT_SECRET\n",
    ")\n",
    "\n",
    "# Use DataLakeServiceClient (correct for ADLS Gen2 directories)\n",
    "account_url = f\"https://{storage_account_name}.dfs.core.windows.net\"  # Note: .dfs not .blob\n",
    "service_client = DataLakeServiceClient(account_url=account_url, credential=credential)\n",
    "\n",
    "# Get file system client (equivalent to container)\n",
    "file_system_client = service_client.get_file_system_client(file_system=container_name)\n",
    "\n",
    "# Ensure the container exists (optional, will fail gracefully if not)\n",
    "try:\n",
    "    file_system_client.create_file_system()  # Idempotent\n",
    "except Exception:\n",
    "    pass  # Already exists\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# MAIN LOOP\n",
    "# ──────────────────────────────────────────────\n",
    "for row in df.itertuples():\n",
    "    try:\n",
    "        item_name = str(row.item_name).strip()\n",
    "        print(f\"\\nProcessing: {item_name}\")\n",
    "\n",
    "        # 1. Create temporary local folder for this item\n",
    "        with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "            local_folder = os.path.join(tmp_dir, \"export\", item_name)\n",
    "            os.makedirs(local_folder, exist_ok=True)\n",
    "\n",
    "            # 2. Export the Fabric item\n",
    "            full_item_path = f\"/{target_workspaceName}/{item_name}\"\n",
    "            print(f\"Exporting {full_item_path} → {local_folder}\")\n",
    "            !fab export \"{full_item_path}\" -f -o \"{local_folder}\"\n",
    "\n",
    "            # 3. Upload everything (skip junk files)\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            remote_prefix = f\"{item_name}_{timestamp}/\"\n",
    "\n",
    "            uploaded_count = 0\n",
    "            for root, _, files in os.walk(local_folder):\n",
    "                for f in files:\n",
    "                    local_path = os.path.join(root, f)\n",
    "                    file_size  = os.path.getsize(local_path)\n",
    "\n",
    "                    # ── SKIP UNWANTED FILES ─────────────────────\n",
    "                    if (not f.strip() or                     # empty filename\n",
    "                        file_size == 0 or                     # zero-byte file\n",
    "                        f.startswith(\".\") or                  # hidden files\n",
    "                        f in {\"__init__.py\", \".gitkeep\", \"desktop.ini\"}):\n",
    "                        print(f\"  Skipping junk file: {f or '<empty name>'}\")\n",
    "                        continue\n",
    "                    # ─────────────────────────────────────────────\n",
    "\n",
    "                    relative_path = os.path.relpath(local_path, local_folder).replace(\"\\\\\", \"/\")\n",
    "                    blob_path = remote_prefix + relative_path\n",
    "\n",
    "                    print(f\"  Uploading {blob_path} ({file_size} bytes)\")\n",
    "                    file_client = file_system_client.get_file_client(blob_path)\n",
    "                    with open(local_path, \"rb\") as data:\n",
    "                        file_client.upload_data(data, overwrite=True)\n",
    "                    uploaded_count += 1\n",
    "\n",
    "            print(f\"Successfully uploaded {item_name} → {container_name}/{remote_prefix} ({uploaded_count} files)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error exporting/uploading '{item_name}': {str(e)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61f559f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 – EXPORT A SINGLE ITEM (change these two lines only)\n",
    "\n",
    "# ←←← CHANGE THESE TWO LINES ONLY\n",
    "workspace_name = \"Fabric_FourMoo.Workspace\"          # your workspace\n",
    "item_name      = \"Blog - Get All Fabric Items - Spark.Notebook \"           # exact name of the item (with suffix)\n",
    "\n",
    "# Where to save it (local temp → OneLake)\n",
    "import tempfile, os\n",
    "from datetime import datetime\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmp:\n",
    "    output_folder = f\"{tmp}/SingleItemExport\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    full_item_path = f\"/{workspace_name}/{item_name}\"\n",
    "    \n",
    "    print(f\"Exporting: {full_item_path}\")\n",
    "    !fab export \"{full_item_path}\" -f -o \"{output_folder}\"\n",
    "    \n",
    "    # Optional: copy straight to OneLake (Files section of the attached lakehouse)\n",
    "    onelake_path = f\"{onelake_base}/SingleItemExports/{item_name}_{datetime.now():%Y%m%d_%H%M%S}\"\n",
    "    notebookutils.fs.mkdirs(onelake_path)\n",
    "    notebookutils.fs.cp(f\"file:{output_folder}\", onelake_path, recurse=True)\n",
    "    \n",
    "    print(f\"\\nDone! Item exported to OneLake:\")\n",
    "    displayHTML(f'<h3><a href=\"{onelake_path}\" target=\"_blank\">Open {item_name} in OneLake</a></h3>')"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "7882a108-ec73-4873-a557-fd1937647ead",
    "default_lakehouse_name": "LH_NewSchema",
    "default_lakehouse_workspace_id": "cb2af739-998e-45c8-8c41-f78d2e8fc1dd",
    "known_lakehouses": [
     {
      "id": "7882a108-ec73-4873-a557-fd1937647ead"
     }
    ]
   }
  },
  "kernel_info": {
   "jupyter_kernel_name": "python3.11",
   "name": "jupyter"
  },
  "kernelspec": {
   "display_name": "Jupyter",
   "name": "jupyter"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "jupyter_python",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
