{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde0cb3e-ad75-4304-8f7a-301412d0d10f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "######################################################################################### \n",
    "# Read secretes from Azure Key Vault\n",
    "#########################################################################################\n",
    "## This is the name of my Azure Key Vault \n",
    "key_vault = \"https://INSERT_HERE.vault.azure.net/\"\n",
    "## I have stored my tenant id as one of the secrets to make it easier to use when needed \n",
    "tenant = mssparkutils.credentials.getSecret(key_vault , \"INSERT_HERE\") \n",
    "## This is my application Id for my service principal account \n",
    "client = mssparkutils.credentials.getSecret(key_vault , \"INSERT_HERE\") \n",
    "## This is my Client Secret for my service principal account \n",
    "client_secret = mssparkutils.credentials.getSecret(key_vault , \"INSERT_HERE\") \n",
    "\n",
    "######################################################################################### \n",
    "# Authentication - Replace string variables with your relevant values \n",
    "#########################################################################################  \n",
    "\n",
    "import json, requests, pandas as pd \n",
    "import datetime  \n",
    "\n",
    "try: \n",
    "    from azure.identity import ClientSecretCredential \n",
    "except Exception:\n",
    "     !pip install azure.identity \n",
    "     from azure.identity import ClientSecretCredential \n",
    "\n",
    "# Generates the access token for the Service Principal \n",
    "api = 'https://analysis.windows.net/powerbi/api/.default' \n",
    "auth = ClientSecretCredential(authority = 'https://login.microsoftonline.com/', \n",
    "               tenant_id = tenant, \n",
    "               client_id = client, \n",
    "               client_secret = client_secret) \n",
    "access_token = auth.get_token(api)\n",
    "access_token = access_token.token \n",
    "\n",
    "## This is where I store my header with the Access Token, because this is required when authenticating \n",
    "## to the Power BI Admin APIs \n",
    "header = {'Authorization': f'Bearer {access_token}'}  \n",
    "\n",
    "print('\\nSuccessfully authenticated.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ae362d-17fc-49b3-af25-b1ff0582c3ae",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Welcome to your new notebook\n",
    "# Type here in the cell editor to add code!\n",
    "# Authenticate with Service Principal\n",
    "\n",
    "# Welcome to your new notebook\n",
    "# Type here in the cell editor to add code!\n",
    "import msal\n",
    "import requests\n",
    "import json\n",
    "import datetime\n",
    "from datetime import datetime,date,timedelta\n",
    "\n",
    "# Replace with your Azure AD app details\n",
    "client_id = client\n",
    "client_secret = client_secret\n",
    "tenant_id = tenant\n",
    "\n",
    "authority_url = f\"https://login.microsoftonline.com/{tenant_id}\"\n",
    "graph_api_url = \"https://graph.microsoft.com/v1.0\"\n",
    "\n",
    "# Create MSAL client application\n",
    "app = msal.ConfidentialClientApplication(\n",
    "    client_id, authority=authority_url, client_credential=client_secret\n",
    ")\n",
    "\n",
    "# Function to get an access token\n",
    "def get_access_token():\n",
    "    token_response = app.acquire_token_for_client(scopes=[\"https://graph.microsoft.com/.default\"])\n",
    "    return token_response.get(\"access_token\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0a7677-621c-48f5-b4ad-0a3457ad728c",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Getting all Groups and Members with Pagination\n",
    "# NOTE: This can take over 30 mins to get all the information.\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import datetime\n",
    "from datetime import datetime,date,timedelta\n",
    "\n",
    "# Replace with your access token\n",
    "access_token = get_access_token()\n",
    "\n",
    "def get_paginated_results(url, headers):\n",
    "    results = []\n",
    "    while url:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        data = response.json()\n",
    "        results.extend(data.get('value', []))\n",
    "        url = data.get('@odata.nextLink')\n",
    "    return results\n",
    "\n",
    "def main():\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {access_token}'\n",
    "    }\n",
    "\n",
    "    # Get all groups\n",
    "    groups_url = 'https://graph.microsoft.com/v1.0/groups'\n",
    "    groups = get_paginated_results(groups_url, headers)\n",
    "\n",
    "    all_groups_members = []\n",
    "\n",
    "    # Get members of each group\n",
    "    for group in groups:\n",
    "        group_id = group['id']\n",
    "        group_name = group['displayName'],\n",
    "        # print(f\"Fetching members for group: {group_name}\")\n",
    "        members_url = f'https://graph.microsoft.com/v1.0/groups/{group_id}/members'\n",
    "        members = get_paginated_results(members_url, headers)\n",
    "        all_groups_members.append({\n",
    "            'group_name': group['displayName'],\n",
    "            'group_id': group['id'],\n",
    "            # 'members': [{'name': member['displayName'], 'id': member['id']} for member in members]\n",
    "            'members':[{\"user_id\": member[\"id\"], \"user_displayName\": member.get(\"displayName\", \"N/A\"),\"userprincipalName\": member.get(\"userprincipalName\", \"N/A\"),\"Email\": member.get(\"mail\", \"N/A\")} for member in members]\n",
    "        })\n",
    "\n",
    "    # Save to file\n",
    "    fileName_groups = 'groups_and_members_' + (datetime.today()).strftime('%Y%m%d') + '.json'\n",
    "\n",
    "    # Write the output to a JSON file\n",
    "    with open(f\"/lakehouse/default/Files/Entra_Id/{fileName_groups}\", \"w\") as json_file:\n",
    "        json.dump(all_groups_members, json_file, indent=4)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1133f55-854b-451a-9a0c-fb90e79136c1",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd \n",
    "from pyspark.sql.functions import col, year, month, quarter, to_date, dayofmonth\n",
    "import datetime\n",
    "from datetime import datetime,date,timedelta\n",
    "#from pandas.io.json import json_normalize \n",
    "\n",
    "with open(f'/lakehouse/default/Files/Entra_Id/groups_and_members_' + (datetime.today()).strftime('%Y%m%d') + '.json') as f:\n",
    "#with open(df, encoding=\"utf-16\") as f:\n",
    "   data = json.load(f)\n",
    "\n",
    "# Get the Expanded Members, by using the json normalize and then have the higher levels of the json structure.\n",
    "\n",
    "df = pd.json_normalize(data, \"members\",['group_id','group_name'])\n",
    "\n",
    "# # # Display the DataFrame\n",
    "sparkDF=spark.createDataFrame(df)\n",
    "# display(sparkDF)\n",
    "\n",
    "# # Write to Table\n",
    "sparkDF.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").saveAsTable('staging_tb_EntraID_Groups')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc54ca1-e0c1-4511-aa76-31eec4cb220e",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Insert or Update New Rows\n",
    "# Source query to create a temporary view\n",
    "groups_source_query = f\"\"\"\n",
    "SELECT user_id,\n",
    "\t\t\tuser_displayName,\n",
    "\t\t\tuserprincipalName,\n",
    "\t\t\tEmail,\n",
    "\t\t\tgroup_id,\n",
    "\t\t\tgroup_name\n",
    "FROM staging_tb_entraid_groups\n",
    "where Email is not null\n",
    "\"\"\"\n",
    "\n",
    "# Create a temporary view from the source query\n",
    "spark.sql(groups_source_query).createOrReplaceTempView(\"groups_updates\")\n",
    "\n",
    "# Merge operation\n",
    "merge_query = f\"\"\"\n",
    "MERGE INTO tb_EntraID_Groups AS dim\n",
    "USING groups_updates AS updates\n",
    "    ON dim.group_id = updates.group_id\n",
    "    and dim.user_displayName = updates.user_displayName\n",
    "    and dim.userprincipalName = updates.userprincipalName\n",
    "    and dim.Email = updates.Email\n",
    "    and dim.user_id = updates.user_id\n",
    "    and dim.group_name = updates.group_name\n",
    "\n",
    "WHEN MATCHED THEN\n",
    "  UPDATE SET\n",
    "    dim.user_id = updates.user_id,\n",
    "    dim.user_displayName = updates.user_displayName,\n",
    "    dim.userprincipalName = updates.userprincipalName,\n",
    "    dim.Email = updates.Email,\n",
    "    dim.group_id = updates.group_id,\n",
    "    dim.group_name = updates.group_name\n",
    "\n",
    "WHEN NOT MATCHED THEN\n",
    "  INSERT (\n",
    "    \n",
    "    user_id,\n",
    "    user_displayName,\n",
    "    userprincipalName,\n",
    "    Email,\n",
    "    group_id,\n",
    "    group_name\n",
    "\n",
    "  ) VALUES (\n",
    "\n",
    "    updates.user_id,\n",
    "    updates.user_displayName,\n",
    "    updates.userprincipalName,\n",
    "    updates.Email,\n",
    "    updates.group_id,\n",
    "    updates.group_name\n",
    "\n",
    "  )\n",
    "\"\"\"\n",
    "\n",
    "# Execute the merge query\n",
    "# Execute the merge query and get the result as a DataFrame\n",
    "result_df = spark.sql(merge_query)\n",
    "\n",
    "# Collect the result to a local variable\n",
    "result = result_df.collect()[0]\n",
    "\n",
    "# Extract the different values\n",
    "num_affected_rows = result['num_affected_rows']\n",
    "num_updated_rows = result['num_updated_rows']\n",
    "num_deleted_rows = result['num_deleted_rows']\n",
    "num_inserted_rows = result['num_inserted_rows']\n",
    "\n",
    "# Print the values\n",
    "print(f\"Number of affected rows: {num_affected_rows}\")\n",
    "print(f\"Number of updated rows: {num_updated_rows}\")\n",
    "print(f\"Number of deleted rows: {num_deleted_rows}\")\n",
    "print(f\"Number of inserted rows: {num_inserted_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5da554e-331a-44c9-a060-33a515adfbd9",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# # Get all Users and download into JSON File\n",
    "# # If needed to get more details here is how: https://stackoverflow.com/questions/48229949/get-all-user-properties-from-microsoft-graph\n",
    "# # Here is an example for the API to get all details: https://graph.microsoft.com/v1.0/groups?$filter=displayname eq 'sec_Office365_FabricAdmin_Role'&$expand=members\n",
    "\n",
    "import requests\n",
    "import msal\n",
    "import datetime\n",
    "from datetime import datetime,date,timedelta\n",
    "\n",
    "# Microsoft Graph API endpoint for listing users\n",
    "GRAPH_API_URL = 'https://graph.microsoft.com/v1.0/users?$select=id,displayName,mail,userPrincipalName,jobTitle,accountEnabled,createdDateTime,lastPasswordChangeDateTime,mailNickName'\n",
    "\n",
    "# Get Access Token\n",
    "\n",
    "access_token = get_access_token()\n",
    "\n",
    "# Define headers for API request\n",
    "headers = {\n",
    "    'Authorization': 'Bearer ' + access_token,\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "# Make the API request to list users\n",
    "response = requests.get(GRAPH_API_URL, headers=headers)\n",
    "\n",
    "# To hold all users\n",
    "all_users = []\n",
    "\n",
    "# Pagination loop\n",
    "while GRAPH_API_URL:\n",
    "    response = requests.get(GRAPH_API_URL, headers=headers)\n",
    "    data = response.json()\n",
    "    \n",
    "    # Process users\n",
    "    all_users.extend(data['value'])\n",
    "    \n",
    "    # Get the next page URL\n",
    "    GRAPH_API_URL = data.get('@odata.nextLink')\n",
    "\n",
    "# Create Directory if it does not exist\n",
    "mssparkutils.fs.mkdirs(\"Files/Entra_Id/\")\n",
    "\n",
    "fileName = 'users_' + (datetime.today()).strftime('%Y%m%d') + '.json'\n",
    "\n",
    "# Write the output to a JSON file\n",
    "with open(f\"/lakehouse/default/Files/Entra_Id/{fileName}\", \"w\") as json_file:\n",
    "    json.dump(all_users, json_file, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cc32ab-44fa-4a21-b0b4-530ee72d317f",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# # Getting the JSON Users File into Dataframe\n",
    "\n",
    "import json\n",
    "import pandas as pd \n",
    "from pyspark.sql.functions import col, year, month, quarter, to_date, dayofmonth\n",
    "import datetime\n",
    "from datetime import datetime,date,timedelta\n",
    "#from pandas.io.json import json_normalize \n",
    "\n",
    "fileName = 'users_' + (datetime.today()).strftime('%Y%m%d') + '.json'\n",
    "\n",
    "with open(f'/lakehouse/default/Files/Entra_Id/{fileName}') as f:\n",
    "#with open(df, encoding=\"utf-16\") as f:\n",
    "   data = json.load(f)\n",
    "\n",
    "# Get the Expanded Members, by using the json normalize and then have the higher levels of the json structure.\n",
    "df = pd.json_normalize(data)\n",
    "\n",
    "# # # Display the DataFrame\n",
    "sparkDF=spark.createDataFrame(df)\n",
    "sparkDF.head(3)\n",
    "\n",
    "# Rename column 'onPremisesExtensionAttributes_extensionAttribute3' to 'PositionCode'\n",
    "# sparkDF = sparkDF.withColumnRenamed('onPremisesExtensionAttributes.extensionAttribute1', 'extensionAttribute1')\n",
    "# sparkDF = sparkDF.withColumnRenamed('onPremisesExtensionAttributes.extensionAttribute2', 'extensionAttribute2')\n",
    "# sparkDF = sparkDF.withColumnRenamed('onPremisesExtensionAttributes.extensionAttribute3', 'PositionCode')\n",
    "# sparkDF = sparkDF.withColumnRenamed('onPremisesExtensionAttributes.extensionAttribute4', 'extensionAttribute4')\n",
    "# sparkDF = sparkDF.withColumnRenamed('onPremisesExtensionAttributes.extensionAttribute5', 'extensionAttribute5')\n",
    "# sparkDF = sparkDF.withColumnRenamed('onPremisesExtensionAttributes.extensionAttribute6', 'extensionAttribute6')\n",
    "# sparkDF = sparkDF.withColumnRenamed('onPremisesExtensionAttributes.extensionAttribute7', 'extensionAttribute7')\n",
    "# sparkDF = sparkDF.withColumnRenamed('onPremisesExtensionAttributes.extensionAttribute8', 'extensionAttribute8')\n",
    "# sparkDF = sparkDF.withColumnRenamed('onPremisesExtensionAttributes.extensionAttribute9', 'extensionAttribute9')\n",
    "# sparkDF = sparkDF.withColumnRenamed('onPremisesExtensionAttributes.extensionAttribute10', 'extensionAttribute10')\n",
    "# sparkDF = sparkDF.withColumnRenamed('onPremisesExtensionAttributes.extensionAttribute11', 'extensionAttribute11')\n",
    "# sparkDF = sparkDF.withColumnRenamed('onPremisesExtensionAttributes.extensionAttribute12', 'extensionAttribute12')\n",
    "# sparkDF = sparkDF.withColumnRenamed('onPremisesExtensionAttributes.extensionAttribute13', 'extensionAttribute13')\n",
    "# sparkDF = sparkDF.withColumnRenamed('onPremisesExtensionAttributes.extensionAttribute14', 'extensionAttribute14')\n",
    "# sparkDF = sparkDF.withColumnRenamed('onPremisesExtensionAttributes.extensionAttribute15', 'extensionAttribute15')\n",
    "\n",
    "# Write Users to Lakehouse Table\n",
    "sparkDF.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").saveAsTable('staging_tb_EntraID_Users')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4787f78c-4088-4855-8439-90572cd0124f",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Insert or Update New Rows for Users\n",
    "# Source query to create a temporary view\n",
    "users_source_query = f\"\"\"\n",
    "SELECT id\n",
    "      ,displayName\n",
    "      ,mail\n",
    "      \n",
    "    \n",
    "      \n",
    "      \n",
    "      \n",
    "      ,createdDateTime\n",
    "      ,lastPasswordChangeDateTime\n",
    "      ,mailNickname\n",
    "      ,userPrincipalName\n",
    "  FROM staging_tb_entraid_users\n",
    "\"\"\"\n",
    "\n",
    "# Create a temporary view from the source query\n",
    "spark.sql(users_source_query).createOrReplaceTempView(\"users_updates\")\n",
    "\n",
    "# Merge operation\n",
    "merge_query = f\"\"\"\n",
    "MERGE INTO tb_EntraID_Users AS dim\n",
    "USING users_updates AS updates\n",
    "\n",
    "    ON dim.id = updates.id\n",
    "    \n",
    "\n",
    "WHEN MATCHED THEN\n",
    "  UPDATE SET\n",
    "\n",
    "    dim.id = updates.id,\n",
    "    dim.displayName = updates.displayName,\n",
    "    dim.mail = updates.mail,\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    dim.createdDateTime = updates.createdDateTime,\n",
    "    dim.lastPasswordChangeDateTime = updates.lastPasswordChangeDateTime,\n",
    "    dim.userPrincipalName = updates.userPrincipalName,\n",
    "    \n",
    "    dim.mailNickname = updates.mailNickname\n",
    "\n",
    "WHEN NOT MATCHED THEN\n",
    "  INSERT (\n",
    "    \n",
    "    id,\n",
    "    displayName,\n",
    "    mail,\n",
    "    \n",
    "    createdDateTime,\n",
    "    \n",
    "    lastPasswordChangeDateTime,\n",
    "   \n",
    "    userPrincipalName,\n",
    "    \n",
    "    mailNickname\n",
    "\n",
    "  ) VALUES (\n",
    "\n",
    "    updates.id,\n",
    "    updates.displayName,\n",
    "    updates.mail,\n",
    "    \n",
    "    updates.createdDateTime,\n",
    "   \n",
    "    updates.lastPasswordChangeDateTime,\n",
    "    \n",
    "    updates.userPrincipalName,\n",
    "   \n",
    "    updates.mailNickname\n",
    "\n",
    "\n",
    "  )\n",
    "\"\"\"\n",
    "\n",
    "# Execute the merge query\n",
    "# Execute the merge query and get the result as a DataFrame\n",
    "result_df = spark.sql(merge_query)\n",
    "\n",
    "# Collect the result to a local variable\n",
    "result = result_df.collect()[0]\n",
    "\n",
    "# Extract the different values\n",
    "num_affected_rows = result['num_affected_rows']\n",
    "num_updated_rows = result['num_updated_rows']\n",
    "num_deleted_rows = result['num_deleted_rows']\n",
    "num_inserted_rows = result['num_inserted_rows']\n",
    "\n",
    "# Print the values\n",
    "print(f\"Number of affected rows: {num_affected_rows}\")\n",
    "print(f\"Number of updated rows: {num_updated_rows}\")\n",
    "print(f\"Number of deleted rows: {num_deleted_rows}\")\n",
    "print(f\"Number of inserted rows: {num_inserted_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e22cbb-ede0-46b8-86b4-babaf2f39dc4",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "################################\n",
    "## Delete Staging Table data\n",
    "################################\n",
    "\n",
    "delete_upnStaging_source_query = f\"\"\"\n",
    "DELETE \n",
    "FROM tb_staging_Entra_ID_User_Licenses\n",
    "\n",
    "\"\"\"\n",
    "#Run SQL Query\n",
    "spark.sql(delete_upnStaging_source_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546a0179-94c4-48eb-acd0-a363b6e7d4e4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "############################################################\n",
    "# # Get all Users Licenses into JSON FILE\n",
    "############################################################\n",
    "\n",
    "import requests\n",
    "import msal\n",
    "import datetime\n",
    "from datetime import datetime,date,timedelta\n",
    "import datetime\n",
    "from pyspark.sql.types import IntegerType,BooleanType,DateType\n",
    "from pyspark.sql.functions import col, year, month, quarter\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "# Get the UserPrincipalName or UPN for all off the active Users\n",
    "\n",
    "# Source query to create a temporary view\n",
    "userprincipalname_source_query = f\"\"\"\n",
    "SELECT \n",
    "\t\t\tuserPrincipalName\n",
    "FROM tb_entraid_users\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#Create the Dataframe for the UPN\n",
    "userprincipalname_result_df = spark.sql(userprincipalname_source_query)\n",
    "\n",
    "# # Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"AddColumnExample\").getOrCreate()\n",
    "\n",
    "# Get the dataframe I want to loop through.\n",
    "# I used the method \"collect()\" to use it as my looping function.\n",
    "data_collect = userprincipalname_result_df.collect()\n",
    "\n",
    "# looping thorough each row of the dataframe\n",
    "for row in data_collect:\n",
    "    \n",
    "    # This is where I am assigning a variable name for each row in my looping dataset.\n",
    "    var_upn = row[\"userPrincipalName\"]\n",
    "\n",
    "    display(var_upn)\n",
    "\n",
    "    try:\n",
    "\n",
    "        # Microsoft Graph API endpoint for listing users\n",
    "        GRAPH_API_URL = f\"https://graph.microsoft.com/v1.0/users/{var_upn}/licenseDetails?$select=skuPartNumber\" \n",
    "\n",
    "        # Get Access Token\n",
    "\n",
    "        access_token = get_access_token()\n",
    "\n",
    "        # Define headers for API request\n",
    "        headers = {\n",
    "            'Authorization': 'Bearer ' + access_token,\n",
    "            'Content-Type': 'application/json'\n",
    "        }\n",
    "\n",
    "        # Make the API request to list users\n",
    "        response = requests.get(GRAPH_API_URL, headers=headers)\n",
    "\n",
    "        # Get the Response from the API\n",
    "        data = response.json()   \n",
    "\n",
    "        # # To hold all users\n",
    "        all_upn_licenses = []\n",
    "\n",
    "        # Adding Additional Rows\n",
    "        all_upn_licenses.extend(data['value'])\n",
    "\n",
    "        #Create Pandas Data Frame\n",
    "        df_license_users = pd.DataFrame(all_upn_licenses)\n",
    "\n",
    "        # #Create SparkDataFrame\n",
    "        spark_license_users = spark.createDataFrame(df_license_users)\n",
    "\n",
    "        # Add Column with UserPrincipalName\n",
    "        spark_license_users = spark_license_users.withColumn(\"userPrincipalName\" , lit(var_upn))\n",
    "        spark_license_users = spark_license_users.withColumn(\"Inserted_DateTime\", lit(current_timestamp()))\n",
    "\n",
    "\n",
    "        # Write Data to Table\n",
    "        spark_license_users.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").format(\"delta\").save(\"Tables/tb_staging_Entra_ID_User_Licenses\")\n",
    "\n",
    "    except Exception as e:\n",
    "        Error: {e}  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d10e0d6-c413-4647-a9c4-877dcbb6a1a5",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "############################################################\n",
    "# # Insert or Update Users with Licenses\n",
    "############################################################\n",
    "\n",
    "# Source query to create a temporary view\n",
    "license_users_source_query = f\"\"\"\n",
    "SELECT skuPartNumber\n",
    "      ,userPrincipalName\n",
    "      ,CAST(Inserted_DateTime as DATE) as Inserted_DateTime\n",
    "  FROM tb_staging_Entra_ID_User_Licenses\n",
    "\"\"\"\n",
    "\n",
    "# Create a temporary view from the source query\n",
    "spark.sql(license_users_source_query).createOrReplaceTempView(\"license_users_updates\")\n",
    "\n",
    "# Merge operation\n",
    "license_users_merge_query = f\"\"\"\n",
    "MERGE INTO tb_Entra_ID_User_Licenses AS dim\n",
    "USING license_users_updates AS updates\n",
    "\n",
    "    ON dim.userPrincipalName = updates.userPrincipalName \n",
    "    AND dim.skuPartNumber = updates.skuPartNumber    \n",
    "\n",
    "WHEN MATCHED THEN\n",
    "  UPDATE SET\n",
    "\n",
    "    dim.userPrincipalName = updates.userPrincipalName,\n",
    "    dim.Inserted_DateTime = updates.Inserted_DateTime,\n",
    "    dim.skuPartNumber = updates.skuPartNumber\n",
    "\n",
    "WHEN NOT MATCHED THEN\n",
    "  INSERT (\n",
    "    \n",
    "    userPrincipalName,\n",
    "    Inserted_DateTime,\n",
    "    skuPartNumber\n",
    "\n",
    "  ) VALUES (\n",
    "\n",
    "    updates.userPrincipalName,\n",
    "    updates.Inserted_DateTime,\n",
    "    updates.skuPartNumber\n",
    "  )\n",
    "\"\"\"\n",
    "\n",
    "# Execute the merge query\n",
    "# Execute the merge query and get the result as a DataFrame\n",
    "license_users_result_df = spark.sql(license_users_merge_query)\n",
    "\n",
    "# Collect the result to a local variable\n",
    "license_users_result = license_users_result_df.collect()[0]\n",
    "\n",
    "# Extract the different values\n",
    "num_affected_rows = license_users_result['num_affected_rows']\n",
    "num_updated_rows = license_users_result['num_updated_rows']\n",
    "num_deleted_rows = license_users_result['num_deleted_rows']\n",
    "num_inserted_rows = license_users_result['num_inserted_rows']\n",
    "\n",
    "# Print the values\n",
    "print(f\"Number of affected rows: {num_affected_rows}\")\n",
    "print(f\"Number of updated rows: {num_updated_rows}\")\n",
    "print(f\"Number of deleted rows: {num_deleted_rows}\")\n",
    "print(f\"Number of inserted rows: {num_inserted_rows}\")"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "environment": {},
   "lakehouse": {
    "default_lakehouse": "260cc545-9a59-45a7-9897-fb705d4bc4ae",
    "default_lakehouse_name": "FM_LH",
    "default_lakehouse_workspace_id": "cb2af739-998e-45c8-8c41-f78d2e8fc1dd"
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  },
  "widgets": {}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
