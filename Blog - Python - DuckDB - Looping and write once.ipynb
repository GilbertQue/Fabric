{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdf07ee-ecbb-42bf-98e4-cccbcd836f4f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "#### Query to get New Data based on Date above and get into DataFrame\n",
    "import duckdb\n",
    "\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Load source and target as Delta tables (replace paths with your lakehouse paths)\n",
    "datehour = \"/lakehouse/default/Tables/datehour/\"\n",
    "\n",
    "# Register Delta tables\n",
    "con.execute(f\"CREATE TABLE datehour AS SELECT * FROM delta_scan('{datehour}')\")\n",
    "\n",
    "# Define the SQL query with CTEs using lakehouse source tables\n",
    "query = \"\"\" \n",
    "Select distinct d.Date \n",
    "from datehour as d\n",
    "where d.Date between '2025-06-01' and '2025-06-03'\n",
    "\"\"\"\n",
    "\n",
    "# Execute the SQL query and fetch results as a DataFrame\n",
    "df_Weather_Date = con.sql(query).fetchdf()\n",
    "\n",
    "\n",
    "# Close the connection\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05500f45-00cf-4dbb-b552-bc812ee858e1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "display(df_Weather_Date)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ead1ae1-5e21-4ed6-83f8-3a74ff3ecdba",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the Unix timestamp (in milliseconds) to datetime\n",
    "df_Weather_Date['Date'] = pd.to_datetime(df_Weather_Date['Date'], unit='ms').dt.date\n",
    "\n",
    "# Display the result\n",
    "print(df_Weather_Date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302f94cc-14f9-4880-a91d-999617af4758",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize an empty list to collect all DataFrames\n",
    "all_weather_dates_dfs = []\n",
    "\n",
    "# display(data_collect)\n",
    "\n",
    "# Loop through each row and collect data\n",
    "for row in df_Weather_Date.itertuples():\n",
    "\n",
    "    # This is where I am assigning a variable name for each row in my looping dataset.\n",
    "    var_Date =str(row.Date)\n",
    "\n",
    "    # Example usage\n",
    "    city = 'London'\n",
    "\n",
    "\n",
    "    # Convert the date to the required format\n",
    "    date_obj = datetime.strptime(var_Date, \"%Y-%m-%d\")\n",
    "    formatted_date = date_obj.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # Open-Meteo API endpoint for historical weather data\n",
    "    url = f\"https://api.open-meteo.com/v1/forecast\"\n",
    "\n",
    "    # Get the latitude and longitude of the city using Open-Meteo's geocoding API\n",
    "    geocode_url = f\"https://geocoding-api.open-meteo.com/v1/search?name={city}\"\n",
    "    response = requests.get(geocode_url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        location_data = response.json()\n",
    "        if location_data and location_data['results']:\n",
    "            lat = location_data['results'][0]['latitude']\n",
    "            lon = location_data['results'][0]['longitude']\n",
    "        else:\n",
    "            print(\"City not found\")        \n",
    "    else:        \n",
    "        print(\"Error fetching location data\")\n",
    "\n",
    "\n",
    "    # Parameters for the API request\n",
    "    params = {\n",
    "        'latitude': lat,\n",
    "        'longitude': lon,\n",
    "        'start_date': formatted_date,\n",
    "        'end_date': formatted_date,\n",
    "        'hourly': 'temperature_2m',\n",
    "        'timezone': 'auto'\n",
    "    }\n",
    "\n",
    "    # Make the API request\n",
    "    response = requests.get(url, params=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        weather_data = response.json()\n",
    "        hourly_data = weather_data['hourly']\n",
    "        \n",
    "        # Create a DataFrame from the hourly data\n",
    "        df_weather = pd.DataFrame({\n",
    "            'date': [formatted_date] * len(hourly_data['time']),\n",
    "            'time': hourly_data['time'],\n",
    "            'temperature': hourly_data['temperature_2m']\n",
    "        })\n",
    "        \n",
    "        print(f\"Weather data for {city} on {var_Date}:\")\n",
    "        print(df_weather)\n",
    "    else:\n",
    "        print(\"Error fetching weather data\")\n",
    "    \n",
    "    # Append to list of DataFrames\n",
    "    all_weather_dates_dfs.append(df_weather)\n",
    "\n",
    "# Combine all DataFrames and write once\n",
    "if all_weather_dates_dfs:\n",
    "    combined_df_weather_Detail = pd.concat(all_weather_dates_dfs, ignore_index=True)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c309fa44-a150-47b0-ab98-f8bd6cbcfe48",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# Converting Date or DateTime to UTC for Correct DateTime Data Type in Lakehouse\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "df_dt_fixed = combined_df_weather_Detail\n",
    "df_dt_fixed['date'] = pd.to_datetime(df_dt_fixed[\"date\"], utc=True)\n",
    "df_dt_fixed['time'] = pd.to_datetime(df_dt_fixed[\"time\"], utc=True)\n",
    "\n",
    "display(df_dt_fixed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9fba2a-b47f-4c96-9084-4b1db6733732",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# Take existing Pandas Data Frame and Write to Lakehouse Table\n",
    "#### NOTE: This includes writing the data with the valid Date Time Column\n",
    "\n",
    "import duckdb\n",
    "from deltalake import write_deltalake\n",
    "import pandas as pd\n",
    "import notebookutils  # Fabric-specific utility\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the Table Name for the Lakehouse\n",
    "table_name = \"weather_London_dt_loop\"\n",
    "\n",
    "# Define Data Frame Name\n",
    "dataframe_name = \"df_dt_fixed\"\n",
    "\n",
    "# Define the Table Mode Overwrite existing table; use \"append\" to add data\n",
    "table_mode = \"overwrite\"\n",
    "\n",
    "# Workspace ID or Workspace GUID\n",
    "workspace_id =\"FILL ME IN\"\n",
    "\n",
    "# Lakehouse ID or Lakehouse GUID\n",
    "lakehouse_id = \"FILL ME IN\"\n",
    "\n",
    "# Define the path to the Lakehouse table (adjust to your workspace and lakehouse)\n",
    "table_path = f\"abfss://{workspace_id}@onelake.dfs.fabric.microsoft.com/{lakehouse_id}/Tables/{table_name}\"\n",
    "\n",
    "\n",
    "# Use DuckDB to query the DataFrame (optional SQL transformation step)\n",
    "# Here, we just select all data, but you could add complex SQL logic\n",
    "duckdb_result = duckdb.sql(f\"SELECT * FROM {dataframe_name}\").arrow()\n",
    "\n",
    "# Write the result to a Delta table in the Lakehouse\n",
    "write_deltalake(\n",
    "    table_path,\n",
    "    duckdb_result,\n",
    "    mode=table_mode,  # Overwrite existing table; use \"append\" to add data\n",
    "    engine=\"rust\"\n",
    ")\n",
    "\n",
    "print(f\"Data successfully written to Lakehouse table! {table_path}\")"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "260cc545-9a59-45a7-9897-fb705d4bc4ae",
    "default_lakehouse_name": "FM_LH",
    "default_lakehouse_workspace_id": "cb2af739-998e-45c8-8c41-f78d2e8fc1dd",
    "known_lakehouses": [
     {
      "id": "260cc545-9a59-45a7-9897-fb705d4bc4ae"
     }
    ]
   }
  },
  "kernel_info": {
   "jupyter_kernel_name": "python3.11",
   "name": "jupyter"
  },
  "kernelspec": {
   "display_name": "Jupyter",
   "language": "Jupyter",
   "name": "jupyter"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "jupyter_python",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
