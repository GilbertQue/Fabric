{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d597c0e4-db4e-484f-9b18-c4cc4ef2f31f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# Welcome to your new notebook\n",
    "# Needed to automatically authenticate to Storage for DuckDB\n",
    "\n",
    "%pip install msfabricutils -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14984797-51c4-4db2-8c25-e7d8a7a909e8",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# Install Semantic Link Labs\n",
    "%pip install semantic-link-labs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae07cd6-3f60-420b-adf5-89d16391a1c1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# Install Semantic Link\n",
    "import pandas as pd\n",
    "import sempy.fabric as fabric\n",
    "from datetime import datetime,date,timedelta\n",
    "\n",
    "!pip install semantic-link -q\n",
    "\n",
    "# Test import to confirm functionality\n",
    "try:\n",
    "    import sempy.fabric as fabric\n",
    "    print(\"Successfully imported sempy.fabric - ready to use!\")\n",
    "except ImportError as e:\n",
    "    print(f\"Failed to import sempy.fabric. Error: {str(e)}\")\n",
    "\n",
    "#Update with names for my Workspace and SemanticModelName\n",
    "WorkspaceName = \"PPU Space Testing\"\n",
    "SemanticModelName = \"WWI Sales - Azure SQL Source - PPU - 48 Months - 2 Days\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5b4ca4-f50e-4cf9-ad11-2a367e1f3535",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# Run DAX Query against Semantic Model\n",
    "import sempy_labs as labs\n",
    "from sempy_labs.tom import connect_semantic_model\n",
    "import sempy.fabric as fabric\n",
    "\n",
    "# DAX query to execute\n",
    "dax_query = \"\"\"\n",
    "EVALUATE\n",
    "'Customer'\n",
    "\"\"\"\n",
    "\n",
    "# Connect to the semantic model\n",
    "with connect_semantic_model(dataset=SemanticModelName, readonly=True, workspace=WorkspaceName) as tom:\n",
    "    # Execute the DAX query using sempy.fabric\n",
    "    df_result = fabric.evaluate_dax(dataset=SemanticModelName, dax_string=dax_query, workspace=WorkspaceName)\n",
    "    \n",
    "# Display the result\n",
    "display(df_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79a4bc5-c323-4cdb-a921-cf7dc1447a94",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# Clean column names\n",
    "df_result.columns = df_result.columns.str.replace(r'(.*\\[)|(\\].*)', '', regex=True)\n",
    "df_result.columns = df_result.columns.str.replace(' ', '_')\n",
    "df_result.columns = df_result.columns.str.replace('(', '')\n",
    "df_result.columns = df_result.columns.str.replace(')', '')\n",
    "\n",
    "# Changing Columns to DateTime\n",
    "df_result['Valid_From'] = pd.to_datetime(df_result[\"Valid_From\"], utc=True)\n",
    "df_result['Valid_To'] = pd.to_datetime(df_result[\"Valid_To\"], utc=True)\n",
    "\n",
    "# Create a cutoff date (1900-01-01)\n",
    "cutoff_date = pd.to_datetime(pd.Timestamp('1900-01-01'), utc=True)\n",
    "\n",
    "# Replace dates before 1900-01-01 with the cutoff date\n",
    "df_result['Valid_To'] = df_result['Valid_To'].apply(lambda x: cutoff_date if x < cutoff_date else x)\n",
    "\n",
    "# Validate Changes \n",
    "display(df_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d068ac9-70e8-47d1-b76c-84893cb99441",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# Write to Customers Table\n",
    "\n",
    "import duckdb\n",
    "from deltalake import write_deltalake\n",
    "import pandas as pd\n",
    "import notebookutils  # Fabric-specific utility\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the Table Name for the Lakehouse\n",
    "table_name = \"Customers\"\n",
    "\n",
    "# Define Data Frame Name\n",
    "dataframe_name = \"df_result\"\n",
    "\n",
    "# Define the Table Mode Overwrite existing table; use \"append\" to add data\n",
    "table_mode = \"overwrite\"\n",
    "\n",
    "# Get the access token for Fabric authentication\n",
    "access_token = notebookutils.credentials.getToken('storage')\n",
    "\n",
    "# Define storage options for Fabric Lakehouse\n",
    "storage_options = {\n",
    "    \"bearer_token\": access_token,\n",
    "    \"use_fabric_endpoint\": \"true\",\n",
    "    \"allow_unsafe_rename\": \"true\"  # Useful for overwriting tables\n",
    "}\n",
    "\n",
    "# Define the path to the Lakehouse table (adjust to your workspace and lakehouse)\n",
    "table_path = f\"/lakehouse/default/Tables/{table_name}\"\n",
    "\n",
    "# Use DuckDB to query the DataFrame (optional SQL transformation step)\n",
    "# Here, we just select all data, but you could add complex SQL logic\n",
    "duckdb_result = duckdb.sql(f\"SELECT * FROM {dataframe_name}\").arrow()\n",
    "\n",
    "# Write the result to a Delta table in the Lakehouse\n",
    "write_deltalake(\n",
    "    table_path,\n",
    "    duckdb_result,\n",
    "    mode=table_mode,  # Overwrite existing table; use \"append\" to add data\n",
    "    engine=\"rust\",     # Use the Rust engine for Delta writing\n",
    "    storage_options=storage_options\n",
    ")\n",
    "\n",
    "print(f\"Data successfully written to Lakehouse table! {table_path}\")"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "260cc545-9a59-45a7-9897-fb705d4bc4ae",
    "default_lakehouse_name": "FM_LH",
    "default_lakehouse_workspace_id": "cb2af739-998e-45c8-8c41-f78d2e8fc1dd",
    "known_lakehouses": [
     {
      "id": "260cc545-9a59-45a7-9897-fb705d4bc4ae"
     }
    ]
   }
  },
  "kernel_info": {
   "jupyter_kernel_name": "python3.11",
   "name": "jupyter"
  },
  "kernelspec": {
   "display_name": "Jupyter",
   "language": "Jupyter",
   "name": "jupyter"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "jupyter_python",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "widgets": {}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
